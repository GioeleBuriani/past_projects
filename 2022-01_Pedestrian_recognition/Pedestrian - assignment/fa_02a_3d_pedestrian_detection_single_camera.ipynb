{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69118e9f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "41fa7d40228a0ae6ec4cfb98c482702e",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Assignment 02a - 3D Pedestrian Detection based on a Single Camera\n",
    "\n",
    "\n",
    "## Goals\n",
    "The goal of this assignment is to obtain **pedestrian locations in the 3D world (camera reference frame)** on the dataset sequence you've been working on in the practica and in [`01a data visualization`](fa_01a_data_visualization.ipynb).\n",
    "\n",
    "You should present a solution which works with a **single camera only**.\n",
    "You are not allowed to make any use of the pedestrian ground truth in the test sequence to improve your detector (e.g. size, appearance, location of pedestrians).\n",
    "Your detector should in principle work equally well on other unseen test sequences (i.e. be generalizable).\n",
    "\n",
    "\n",
    "Your approach should work in the following conditions:\n",
    "- minimum distance of pedestrian to camera: 5 m\n",
    "- maximum distance of pedestrian to camera: 40 m\n",
    "- minimum pedestrian height: 1.2 m\n",
    "- maximum pedestrian height: 1.9 m\n",
    "\n",
    "All notes mentioned in [`00 overview`](fa_00_overview.ipynb) apply, such as the policy on code reuse.\n",
    "\n",
    "## Input\n",
    "As in [`01a data visualization`](fa_01a_data_visualization.ipynb), you will work with the custom `Sequence` of `Dataset` with `start_index=1430` and `end_index=1545`.\n",
    "\n",
    "## Output\n",
    "- Plots, visualizations and videos within this notebook (but please clear the outputs before handing in the ipynb files)\n",
    "- Answers to the questions within this notebook\n",
    "- Evaluation metrics and plots representing the performance of your approach\n",
    "- A dictionary called `frame_pedestrian_dicts` of the format:\n",
    "\n",
    "```python\n",
    "{\n",
    "1430: [\n",
    "  {'label_class': 'Pedestrian',\n",
    "   \"extent_object\": array([0.41, 0.55, 1.72]),\n",
    "   'T_cam_object': array([[-0.013857  , -0.9997468 ,  0.01772762,  6.00276488],\n",
    "          [ 0.10934269, -0.01913807, -0.99381983,  2.63204144],\n",
    "          [ 0.99390751, -0.01183297,  0.1095802 , 10.77843551],\n",
    "          [ 0.        ,  0.        ,  0.        ,  1.        ]]),\n",
    "   'score': 0.851},\n",
    "  {'label_class': 'Pedestrian',\n",
    "   \"extent_object\": array([0.5, 0.5, 1.7]),\n",
    "   'T_cam_object': array([[-1.38569996e-02, -9.99746799e-01,  1.77276209e-02,\n",
    "           -1.47727574e+01],\n",
    "          [ 1.09342687e-01, -1.91380698e-02, -9.93819833e-01,\n",
    "            1.06022264e+01],\n",
    "          [ 9.93907511e-01, -1.18329702e-02,  1.09580196e-01,\n",
    "            2.95320115e+01],\n",
    "          [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
    "            1.00000000e+00]]),\n",
    "   'score': 0.56},\n",
    "    ...\n",
    "   ],\n",
    "1431: [\n",
    "  {'label_class': 'Pedestrian',\n",
    "   \"extent_object\": array([0.41, 0.55, 1.72]),\n",
    "   'T_cam_object': array([[-0.013857  , -0.9997468 ,  0.01772762,  6.00276488],\n",
    "          [ 0.10934269, -0.01913807, -0.99381983,  2.63204144],\n",
    "          [ 0.99390751, -0.01183297,  0.1095802 , 10.77843551],\n",
    "          [ 0.        ,  0.        ,  0.        ,  1.        ]]),\n",
    "   'score': 0.851},\n",
    "    ...\n",
    "   ],\n",
    "}\n",
    "```\n",
    "The keys represent the frame_index of the `Measurements` within the `Sequence`, each value is a list of `pedestrian_dict`s which resemble the format of the annotated 3D object labels (cf. `Measurements.get_labels_camera()`):\n",
    "- `label_class`: should be `'Pedestrian'` for all objects\n",
    "- `extent_object` describes the object extent in `[length, width, height]` (along object's `[x, y, z]` axes, respectively). Cf. Practicum 1.\n",
    "- `T_cam_object` describes the transformation from the `object` local frame (center bottom of object) into the camera frame `cam` as a homogeneous transformation matrix. The orientation of the object (given by the top-left 3x3 sub-matrix of `T_cam_object` does not matter. You can use the identity (`np.eye(3)`). The submatrix `T_cam_object[0:3, 3]` describes the bottom center point of the object in camera frame.\n",
    "- `score`: a floating point number between 0.0 and 1.0 representing the objectness of the object, i.e. the probability of being a pedestrian (as opposed to background).\n",
    "\n",
    "You can use other custom keys with further information in case you want to use it in a later notebook, but it should be json serializable (i.e., convert numpy arrays to python lists, first). \n",
    "\n",
    "Please use\n",
    "```python\n",
    "from assignment.solution_helpers import save_frame_pedestrian_dicts\n",
    "save_frame_pedestrian_dicts(frame_pedestrian_dicts)\n",
    "```\n",
    "to serialize `frame_pedestrian_dicts` to file (to be potentially loaded in a successive assignment notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c4eab9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8769b4c975d8bd1df0297cac29f5e451",
     "grade": false,
     "grade_id": "specification-intended-solution",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Q 02a.1 Specification of intended solution\n",
    "In this notebook, you should implement an approach that could potentially run within a vehicle which is only equipped with a **single (mono) camera**, i.e. without LiDAR, radar or stereo/disparity information.\n",
    "\n",
    "Please describe what you will implement to achieve the goal of obtaining 3D pedestrian locations via a single camera.\n",
    "Here are some questions to stimulate the specification of your solution:\n",
    "1. What processing steps and building blocks are needed?\n",
    "2. Which concepts from the lectures are you planning to incorporate?\n",
    "3. Which building blocks from the practica and assignment 01a will you be re-using?\n",
    "4. Which intermediate results can you represent in plots/visualizations/tables to show the graders that your intended solution does the right thing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d599fe1d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0981d44d449910e050e0495b0ef95bd0",
     "grade": true,
     "grade_id": "concepts-intended-solution-answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "### A 02a.1\n",
    "**Your answer:** (maximum 400 words)\n",
    "1. First of all, the detection phase must be faced. Since there is no 3D detection aid for this problem, a more primitive approach will be used. In particular, the information about the ground plane will be taken into accountto produce smarter proposal boxes with respect to a simple sliding window approach, especially in terms of size. After the image has been split in different smaller patches, the classification phase begins. As for the image patch classification, since only the given sequence can be used, and that should also be our test set, the most reasonable solution consists in using a pretrained feature extractore and classifier. After that, the positive detections will to be \"brought back\" to the 3D environment in order to store the 3D position of the pedestrians.\n",
    "2. Since we are working with a single camera only, all the knowledge regarding the monocular vision will be fundamental, including the perspective camera model and the two types of transformations. Obviously, also all the information regarding the techniques for region proposal generation will be useful, especially the part about proposals without 3D knowledge (except for the ground plane). Unfortunately, for the classifier part, not much knowledge can be used, due to the lack of a training test.\n",
    "3. Practicum 1 has a very similar intent compared to this assignment, therefore a great part of procedure steps and code from that assignment will be extremely useful. In particular the two functions `project_points` and `BoundingBox` from `practicum1` will be fundamental. Other than those, also the function `get_plane_meshgrid` from `fa_01a_data_visualization` will be used to create an initial meshgrid for the proposal boxes.\n",
    "4. The plots and visualizations I intend to share are mostly k3d plots and images, to show the working in the 3D and 2D space respectively. Since the first part concerns a creation of possible boxes in 3D world, a k3d plot will be shown, followed by its results on the 2D world, that will all be represented by images. Finally, some considerations about the last part will again be represented as a k3d plot since they belong to the 3D environment. \n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d32163c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "448cd5167a5e2b6ce6835294854db805",
     "grade": false,
     "grade_id": "proactive-reflection",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q 02a.2 Proactive reflection\n",
    "1. Which assumptions does your intended solution make?\n",
    "2. In which situations might your intended solution fail?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12159655",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7456d32820420633dc439b42a0bc073",
     "grade": true,
     "grade_id": "concepts-proactive-reflection-answer",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "### A 02a.2\n",
    "**Your answer:** (maximum 200 words)\n",
    "1. The main assumption on which my solution is based on is that the proposal boxes fit decently well the pedestrians. Since there is no additional aid from lidars or radars, in order to build the detection boxes there must be some sort of systematic discrete method that will obviously not be able to perfectly fit the pedestrians every time. Therefore the assumption is that the information provided at the end are enough for the scope of this problem.\n",
    "2. Connecting to the previous answer, my solution is not efficient if the information in the final dictionary needs to be very precise both in terms of position and of dimension of the object. In fact the dimension will be kept at a standard since there is no way to understanding automatically if the detection box underfits or overfits the pedestrian. The position is probably slightly more precise, but once again is influenced by the box that will be defined as \"best fitting\".\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fc9a12",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ad4714288cca9a21d9300c00b01ffa7f",
     "grade": false,
     "grade_id": "have-fun",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# now: HAVE FUN & HAPPY CODING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad04f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some magic to ease iterative implementation\n",
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "if ipython:\n",
    "    ipython.magic(\"load_ext autoreload\")\n",
    "    ipython.magic(\"autoreload 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f07a65",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2f3208843c48882377a6d08078bc39b",
     "grade": false,
     "grade_id": "ipynb-hints",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If you want to import functions, e.g., from `fa_01a_data_visualization.ipynb` notebook, you can import them as shown below.\n",
    "If you run into any errors when importing via `ipynb`, please check the hints within [`00 Overview`](fa_00_overview.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0471a61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.fa_01a_data_visualization import get_bounding_box_from_object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5a4fa2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dea9e66e99fee94f7958b9776e4169a2",
     "grade": false,
     "grade_id": "methonds-not-to-use",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": false
   },
   "source": [
    "As stated above, you should provide a single-camera only solution.\n",
    "So technically speaking, your approach **should not use the following methods** from the `Measurements` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cd9799",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67df1d0c65355aca976a930314581b9a",
     "grade": false,
     "grade_id": "code-methods-not-to-use",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sequence_loader import Measurements\n",
    "\n",
    "forbidden_method_tokens = {\"radar\", \"lidar\", \"right\", \"disparity\"}\n",
    "[method_name for method_name in dir(Measurements) if any(token in method_name for token in forbidden_method_tokens)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f35f7a2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23a6a3167779b09a3966587dc242341d",
     "grade": false,
     "grade_id": "pedestrian-detector-class",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## PedestrianDetector class\n",
    "For your convenience, we created a class `PedestrianDetector` you should use as a base-class for your implementation.\n",
    "\n",
    "A `PedestrianDetector` encapsulates the functionality of obtaining 3D pedestrian locations in camera frame from a `Measurements` object, i.e. the data of sensors for a single point in time.\n",
    "When instantiating with `is_debug=True`, it fills the `DebugOutputAggregator` (`self.doa`) with debug outputs, such as visualizations to be retrieved and visualized after detection.\n",
    "This becomes handy for demonstrating intermediate steps of your approach. Setting `is_debug=False` allows to skip the time-consuming visualizations and allows for faster execution.\n",
    "After instantiating an object of the class, the intended steps are feeding a `Measurements` object with `set_measurements()` and obtaining a list of `pedestrian_dict`s by calling `get_pedestrian_dicts()`.\n",
    "\n",
    "Please get familiar with the `PedestrianDetector` class below. And don't worry: we'll show you an example further down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee9dc04",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c67bbad8acdc03e5e5c4f1c7d59e2af",
     "grade": false,
     "grade_id": "code-pedestrian-detector-class",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from assignment.solution_helpers import DebugOutputAggregator\n",
    "from common.sequence_loader import Measurements\n",
    "\n",
    "\n",
    "class PedestrianDetector:\n",
    "    \"\"\"\n",
    "    Pedestrian Detector class.\n",
    "\n",
    "    Make a subclass of me which fulfills my interface.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, is_debug=False):\n",
    "        \"\"\"\n",
    "        is_debug: whether to add debug output (plots, figures, images, strings) to self.doa\n",
    "        \"\"\"\n",
    "        self.is_debug = is_debug\n",
    "        self.doa = DebugOutputAggregator()\n",
    "        self.measurements = None\n",
    "\n",
    "    def set_measurements(self, measurements: Measurements):\n",
    "        \"\"\"\n",
    "        measurements: Measurements object of the frame to detect pedestrians in.\n",
    "        \"\"\"\n",
    "        self.measurements = measurements\n",
    "\n",
    "        # clear debug output aggregator\n",
    "        self.doa = DebugOutputAggregator()\n",
    "\n",
    "    def get_pedestrian_dicts(self):\n",
    "        \"\"\"\n",
    "        Perform 3D pedestrian detection on the the current Measurements.\n",
    "\n",
    "        return list of pedestrian_dicts\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f6bbf8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15ec31c9f21408672a8a520bd6f89210",
     "grade": false,
     "grade_id": "pedestrian-detector-example-subclass",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## PedestrianDetector example subclass\n",
    "To show you how to create a subclass of the `PedestrianDetector`, we provide you with the following example below. Please also note how the `DebugOutputAggregator` is being used to add visualizations for debugging and grading (such as k3d plots, Matplotlib figures, images or strings).\n",
    "Please note that visualizations are only added to `self.doa` (`doa`: short for `DebugOutputAggregator`), if `self.is_debug` is set to `True`.\n",
    "You should do the same with your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a8a3f5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd0ce331721f690131c7b578f28087dd",
     "grade": false,
     "grade_id": "oracle-pedestrian-detector",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import k3d\n",
    "import matplotlib.pyplot as plt\n",
    "from common.k3d_helpers import plot_axes, plot_box\n",
    "\n",
    "\n",
    "class OraclePedestrianDetector(PedestrianDetector):\n",
    "    \"\"\"\n",
    "    Oracle Pedestrian Detector as a dummy implementation.\n",
    "\n",
    "    This class demonstrates how to make a subclass of PedestrianDetector and use\n",
    "    the DebugOutputAggregator to show intermediate results, if is_debug is set to True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, is_debug):\n",
    "        # propagate is_debug flag to mama.\n",
    "        super().__init__(is_debug)\n",
    "\n",
    "    def get_pedestrian_dicts(self):\n",
    "        \"\"\"\n",
    "        This function returns the ground truth pedestrian dicts as detections.\n",
    "\n",
    "        Returning ground truth as detections is kinda cheating, but we do it for demonstration purposes.\n",
    "        It also shows you that detections and ground truth labels should have the same representation\n",
    "        to allow for reuse of visualization functions\n",
    "        \"\"\"\n",
    "        # get ground truth labels\n",
    "        label_dicts = self.measurements.get_labels_camera()\n",
    "        # filter pedestrians\n",
    "        pedestrian_dicts = [ld for ld in label_dicts if ld[\"label_class\"] == \"Pedestrian\"]\n",
    "\n",
    "        # remove bottom_center_cam to be json serializable later on\n",
    "        for pedestrian_dict in pedestrian_dicts:\n",
    "            del pedestrian_dict[\"bottom_center_cam\"]\n",
    "\n",
    "        # the following lines demonstrate the usage of DebugOutputAggregator to visualize intermediate results\n",
    "        # as strings, k3d plots, matplotlib figures or images\n",
    "\n",
    "        if self.is_debug:\n",
    "            # add example string\n",
    "            self.doa.add_string(\n",
    "                **{\n",
    "                    \"name\": \"description-of-approach\",\n",
    "                    \"description\": \"Description of the approach of OraclePedestrianDetector\",\n",
    "                    \"string\": \"The OraclePedestrianDetector publishes the ground truth pedestrians as detections.\\n\"\n",
    "                    \"We make use of the fact that ground truth labels and detections are represented in the same format.\\n\"\n",
    "                    \"Using ground truth objects as detections within *your* solution does not earn you any points.\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if self.is_debug:\n",
    "            # add k3d plot with ground truth objects\n",
    "            plot = k3d.plot(camera_auto_fit=False)\n",
    "            plot.camera = [0.99, -7.18, -6.40, 0.19, -1.16, 4.17, -0.06, -0.89, 0.44]\n",
    "            plot += plot_axes()  # camera frame\n",
    "            # origin frame of k3d plot is the camera fram, so use identity to transform labels\n",
    "            T_origin_camera = np.eye(4, dtype=np.float32)\n",
    "            color_red = 0xFF0000\n",
    "            for pedestrian_dict in pedestrian_dicts:\n",
    "                plot_box(plot, pedestrian_dict, T_origin_camera=T_origin_camera, color=color_red)\n",
    "            # add the k3d plot to debug output aggregator\n",
    "            self.doa.add_k3d_plot(\n",
    "                **{\n",
    "                    \"name\": \"ground-truth-pedestrian-dicts\",\n",
    "                    \"description\": \"A plot showing ground truth pedestrian boxes in red.\\n\"\n",
    "                    f\"There are {len(pedestrian_dicts)} ground truth objects in the scene.\\n\"\n",
    "                    \"The origin of the plot is the camera frame.\",\n",
    "                    \"plot\": plot,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if self.is_debug:\n",
    "            # add bar plot of pedestrian heights\n",
    "            fig, ax = plt.subplots()\n",
    "            extents = np.asarray([pedestrian_dict[\"extent_object\"] for pedestrian_dict in pedestrian_dicts])\n",
    "            ax.bar(range(extents.shape[0]), extents[:, 2])\n",
    "            ax.set_xlabel(\"ground truth pedestrian id\")\n",
    "            ax.set_ylabel(\"height of pedestrian [m]\")\n",
    "            ax.set_xticks(range(extents.shape[0]))\n",
    "            plt.close(fig)\n",
    "            self.doa.add_matplotlib_figure(\n",
    "                **{\n",
    "                    \"name\": \"bar-chart-of-pedestrian_heights\",\n",
    "                    \"description\": \"Heights of each pedestrian ground truth object in the scene\",\n",
    "                    \"figure\": fig,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if self.is_debug:\n",
    "            # add example image\n",
    "            image_draw = self.measurements.get_camera_image()\n",
    "            uv_image_center = (image_draw.shape[1] // 2, image_draw.shape[0] // 2)\n",
    "            cv2.circle(image_draw, uv_image_center, radius=20, color=(0, 255, 255), thickness=2)\n",
    "            self.doa.add_image(\n",
    "                **{\n",
    "                    \"name\": \"image-with-circle\",\n",
    "                    \"description\": \"An example image showing how to add images to the DebugOutputAggregator.\\n\"\n",
    "                    \"It contains a yellow circle in the image center.\",\n",
    "                    \"image\": image_draw,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # we make use of the fact that labels and detections are to be represented in the same way\n",
    "        return pedestrian_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06f7b08",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0de83670dbccf6157eba69034973cb8",
     "grade": false,
     "grade_id": "run-oracle-pedestrian-detector-on-single-frame",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's instantiate an `OraclePedestrianDetector` and run it on a single Measurements object to obtain `pedestrian_dicts` and look at the debug output afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88d44c3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "716d98a9e098679ebcfa8b5d67fccd8e",
     "grade": false,
     "grade_id": "code-run-oracle-pedestrian-detector-on-single-frame",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# run OraclePedestrianDetector on a single frame\n",
    "from common.sequence_loader import Dataset\n",
    "\n",
    "dataset = Dataset()\n",
    "start_index = 1430\n",
    "end_index = 1545\n",
    "sequence = dataset.get_custom_sequence(start_index, end_index)\n",
    "\n",
    "measurements = next(iter(sequence))\n",
    "pedestrian_detector = OraclePedestrianDetector(is_debug=True)\n",
    "pedestrian_detector.set_measurements(measurements)\n",
    "pedestrian_dicts = pedestrian_detector.get_pedestrian_dicts()\n",
    "pedestrian_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baef7bd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bbfe99b082d8007c60f7970d48005a52",
     "grade": false,
     "grade_id": "doa-oracle-pedestrian-detector",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# iterate over all DetectionOutputAggregator objects and show them accordingly below this cell\n",
    "# this is the way we will look at your intermediate results as well\n",
    "# it is important to us, that you create sufficient intermediate results\n",
    "# and also use verbose descriptions of the debug outputs\n",
    "# (as you would use for captions of figures in scientific papers)\n",
    "#\n",
    "# you can toggle scrolling of the output by selecting this cell and 'Cell' > 'Current Outputs' > 'Toggle Scrolling'\n",
    "[None for i in iter(pedestrian_detector.doa)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd73c1f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8cdeb0d18a845608aba40674f2f487b8",
     "grade": false,
     "grade_id": "impl-own-pedestrian-detector-task",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "scrolled": false
   },
   "source": [
    "## Your own PedestrianDetector\n",
    "Now it's time to create your own subclass of `PedestrianDetector` and detect pedestrians in 3D in the camera frame.\n",
    "Please provide visualizations of a few intermediate steps in order to obtain partial credit for concepts/implementation and to show the graders that your approach provides the intended functionality.\n",
    "\n",
    "We recommend to start with a simple approach and iteratively improve it based on the experience you gain along the line.\n",
    "\n",
    "You can reuse/copy code and files from the practica from your own group.\n",
    "If you copy code, please mark it as such, e.g. by adding a comment above denoting where you copied it from.\n",
    "If you load files, you have to use the environment variable `SOURCE_DIR`, e.g., `os.path.join(os.environ[\"SOURCE_DIR\"], \"practicum1\", \"file_name\")`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ee13e1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5db68d539e59579723d5d9d533e7d6fc",
     "grade": false,
     "grade_id": "impl-own-pedestrian-detector-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create your subclass of PedestrianDetector here\n",
    "# Then instantiate it to an object called `pedestrian_detector`\n",
    "# and feed it with a single measurement of the provided sequence\n",
    "\n",
    "from common.sequence_loader import Dataset\n",
    "\n",
    "from ipynb.fs.defs.fa_01a_data_visualization import get_plane_meshgrid\n",
    "from ipynb.fs.defs.practicum1 import project_points\n",
    "from ipynb.fs.defs.practicum1 import BoundingBox\n",
    "from common.visualization import draw_bbox_to_image\n",
    "from common.visualization import showimage\n",
    "from ImagePatch import ImagePatch\n",
    "from BoundingBox import clip_bbox_to_image\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from preprocessing_fns import preprocessing_fn_mobilenet\n",
    "from tensorflow.image import non_max_suppression\n",
    "\n",
    "is_debug = True\n",
    "\n",
    "# class MyFancyPedestrianDetector(PedestrianDetector):\n",
    "#     ...\n",
    "#\n",
    "# pedestrian_detector = MyFancyPedestrianDetector(is_debug=is_debug)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "class MyFancyPedestrianDetector(PedestrianDetector):\n",
    "    \n",
    "    # Copied the init function from the example above\n",
    "    def __init__(self, is_debug):\n",
    "        super().__init__(is_debug)\n",
    "        \n",
    "    # Define a variant of get_plane_meshgrid from fa_01a_data_visualization that deals with ground_plane in camera frame\n",
    "    def get_plane_meshgrid_cf(plane_model, x_min, x_max, z_min, z_max, x_step, z_step):\n",
    "        a, b, c, d = plane_model\n",
    "        x = np.arange(x_min, x_max + x_step, x_step)\n",
    "        z = np.arange(z_min, z_max + z_step, z_step)\n",
    "        Xs, Zs = np.meshgrid(x, z)\n",
    "        Ys = (a * Xs + c * Zs + d) / (- b)\n",
    "        return Xs, Ys, Zs\n",
    "    \n",
    "    # Filter the points of the meshgrid so to keep only those who fit in the camera image\n",
    "    def filter_points(image, points, projection_matrix):\n",
    "        # Augment points dimension and project them in 2D\n",
    "        points_aug = np.hstack((points, np.ones((points.shape[0], 1))))\n",
    "        uvs = project_points(projection_matrix, points_aug)\n",
    "        # Filter the points whose 2D projection lands outside the camera image\n",
    "        points_reduced = points[uvs[:, 0] >= 0]\n",
    "        uvs_reduced = uvs[uvs[:, 0] >= 0]\n",
    "        points_reduced = points_reduced[uvs_reduced[:, 0] < image.shape[1]]\n",
    "        uvs_reduced = uvs_reduced[uvs_reduced[:, 0] < image.shape[1]]\n",
    "        points_reduced = points_reduced[uvs_reduced[:, 1] >= 0]\n",
    "        uvs_reduced = uvs_reduced[uvs_reduced[:, 1] >= 0]\n",
    "        points_reduced = points_reduced[uvs_reduced[:, 1] < image.shape[0]]\n",
    "        uvs_reduced = uvs_reduced[uvs_reduced[:, 1] < image.shape[0]]\n",
    "        return points_reduced\n",
    "    \n",
    "    # Define a function that calculates the corners of the bounding box in 3D environment\n",
    "    def corners_from_grid(grid_points, height, width):\n",
    "        corners = []\n",
    "        for i in range(len(grid_points)):\n",
    "            corners.append([grid_points[i, 0] - width/2, grid_points[i, 1], grid_points[i, 2]])\n",
    "            corners.append([grid_points[i, 0] + width/2, grid_points[i, 1], grid_points[i, 2]])\n",
    "            corners.append([grid_points[i, 0] - width/2, grid_points[i, 1] - height, grid_points[i, 2]])\n",
    "            corners.append([grid_points[i, 0] + width/2, grid_points[i, 1] - height, grid_points[i, 2]])\n",
    "        return np.array(corners)\n",
    "    \n",
    "    \n",
    "    # This function generates all the BoundingBox objects to extract the image patches from the image.\n",
    "    # Rather than using a simple sliding window approach, this procedure first builds a series of possible regions in the 3D\n",
    "    # environment where people may be, later it projects those points in the image to get the region proposal. This method is\n",
    "    # based on the knowledge of the ground plane equation that it uses to identify a series of possible pedestrian positions in\n",
    "    # the image. The advantage of this method with respect to the sliding window approach is that it automatically generates\n",
    "    # region proposals of different dimension based on the distance from the camera, and it only proposes regions \"standing\" on\n",
    "    # the ground plane.\n",
    "    def get_bbox(self):\n",
    "        \n",
    "        # Get the ground plane from the current measurements\n",
    "        ground_plane = self.measurements.get_ground_plane()\n",
    "        \n",
    "        # Create a meshgrid to identify all the possible points on the ground plane that will be analysed\n",
    "        # Define parameters for the meshgrid\n",
    "        z_min = 5\n",
    "        z_max = 40\n",
    "        x_min = -50   # We can choose very large margins since the points will be filtered later\n",
    "        x_max = 50\n",
    "        z_step = 1   # The smaller this interval the more optimized the algorithm (step=0.5 takes 15.89s, step=1 takes 7.23s)\n",
    "        x_step = 1\n",
    "        # Get points of ground plane\n",
    "        Xs, Ys, Zs = MyFancyPedestrianDetector.get_plane_meshgrid_cf(ground_plane, x_min, x_max, z_min, z_max, x_step, z_step)\n",
    "        XYZs = np.vstack([Xs.ravel(), Ys.ravel(), Zs.ravel()]).T\n",
    "        \n",
    "        # Get the image and the projection matrix from measurements\n",
    "        image = self.measurements.get_camera_image()\n",
    "        projection_matrix = self.measurements.get_camera_projection_matrix()\n",
    "        # Filter points that fit in camera image\n",
    "        XYZs_filtered = MyFancyPedestrianDetector.filter_points(image, XYZs, projection_matrix)\n",
    "        \n",
    "        # Calculate the corners for the bounding boxes in 3D world\n",
    "        corners_3d = MyFancyPedestrianDetector.corners_from_grid(XYZs_filtered, 1.70, 0.8)\n",
    "        \n",
    "        # Plot the meshgrid with the corresponding corners in the 3D world\n",
    "        if self.is_debug:\n",
    "            plot = k3d.plot()\n",
    "            plot += plot_axes(np.eye(4, dtype=np.float32))\n",
    "            plot += k3d.points(positions=XYZs_filtered.astype(np.float32), point_size=0.2, color=0xff0000)\n",
    "            plot += k3d.points(positions=corners_3d.astype(np.float32), point_size=0.1, color=0x00ff00)\n",
    "#             plot.display()\n",
    "            self.doa.add_k3d_plot(\n",
    "                **{\n",
    "                    \"name\": \"Possible detections in 3D world\",\n",
    "                    \"description\": \"A plot showing all then possible detections represented in a 3D environment. \\n\"\n",
    "                    \"Notice the cone shaped disposition due to the filtering of the points.\",\n",
    "                    \"plot\": plot,\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # Project the points in 2D\n",
    "        corners_3d_aug = np.hstack((corners_3d, np.ones((corners_3d.shape[0], 1))))        \n",
    "        corners_2d = project_points(projection_matrix, corners_3d_aug)\n",
    "        # Reshape the array to get the corners of each bounding box per row\n",
    "        corners_2d = corners_2d.reshape(-1, 4, 2)\n",
    "        \n",
    "        # Create a list with a bounding box for each set of corners\n",
    "        bbox_list = []\n",
    "        for i in range(corners_2d.shape[0]):\n",
    "            bbox = BoundingBox(corners_2d[i, 2, 1].astype(np.int32), corners_2d[i, 2, 0].astype(np.int32), corners_2d[i, 1, 1].astype(np.int32), corners_2d[i, 1, 0].astype(np.int32), from_corners=True)\n",
    "            bbox_list.append(bbox)\n",
    "        \n",
    "        # Print the image with all the possible bounding boxes\n",
    "        if self.is_debug:\n",
    "            image_test = self.measurements.get_camera_image()\n",
    "            for i in range(len(bbox_list)):\n",
    "                draw_bbox_to_image(image_test, bbox_list[i])\n",
    "#             showimage(image_test)\n",
    "            self.doa.add_image(\n",
    "                **{\n",
    "                    \"name\": \"All possible bboxes\",\n",
    "                    \"description\": \"The image shows in the 2D environment all the bounding boxes that will be considered for the detection.\\n\",\n",
    "                    \"image\": image_test,\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # Finally, crate the proposals based on the bounding boxes\n",
    "        frame_proposals = []\n",
    "        for i in range(len(bbox_list)):\n",
    "            clip_bbox_to_image(bbox_list[i], image.shape[:2])\n",
    "            patch_image = image[bbox_list[i].v:bbox_list[i].v+bbox_list[i].h, bbox_list[i].u:bbox_list[i].u+bbox_list[i].w]\n",
    "            patch = ImagePatch(patch_image, bbox_list[i])\n",
    "            frame_proposals.append(patch)\n",
    "            \n",
    "        return [XYZs_filtered, frame_proposals]\n",
    "    \n",
    "    # Simply apply the pretrained classifier on the proposals and identify the ones with pedestrian.\n",
    "    # Great part of the following code has been copied from practicum1.\n",
    "    def find_pedestrian(self):\n",
    "        \n",
    "        # First, define the classifier. A pretrained classifier willbe used due to lack of training set.\n",
    "        patch_classifier = tf.keras.models.load_model(os.path.join(os.environ[\"SOURCE_DIR\"], \"practicum1\", \"pedestrian_classifier\"))\n",
    "        \n",
    "        # Get the frame_proposals with get_bbox function\n",
    "        XYZs_filtered, frame_proposals = MyFancyPedestrianDetector.get_bbox(self)\n",
    "        \n",
    "        # Preprocess the patches and classify them\n",
    "        frame_patches = np.concatenate([preprocessing_fn_mobilenet(proposal_patch.image) for proposal_patch in frame_proposals], 0)\n",
    "        predictions = patch_classifier.predict(frame_patches)\n",
    "        # Add the score feature of each patch to its description\n",
    "        for i, pred in enumerate(predictions):\n",
    "            frame_proposals[i].score = pred\n",
    "        # Filter the patches to estract only those representing pedestrian with high certainty\n",
    "        threshold = 0.6\n",
    "        pedestrian_patches = [proposal_patch for proposal_patch in frame_proposals if proposal_patch.score >= threshold]\n",
    "        \n",
    "        # Show all the pedestrian predictions in the image\n",
    "        if self.is_debug:\n",
    "            image_test_1 = self.measurements.get_camera_image()\n",
    "            for pedestrian in pedestrian_patches:\n",
    "                bbox = pedestrian.bbox\n",
    "                draw_bbox_to_image(image_test_1, bbox, color=(255,0,0))\n",
    "#             showimage(image_test_1)\n",
    "            self.doa.add_image(\n",
    "                **{\n",
    "                    \"name\": \"Pedestrian patches in the image\",\n",
    "                    \"description\": \"All the patches classiffied as containing a pedestrian are shown in the image\\n\",\n",
    "                    \"image\": image_test_1,\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # Reduce the overlapping proposals to a single one using NMS algorithm .\n",
    "        # I had to modify the code from practicum 1 since that only generated one patch even for two distinct pedestrians.\n",
    "        all_bboxes = []\n",
    "        confidences = []\n",
    "        nms_patches = []\n",
    "        overlap_thresh = 0.01\n",
    "        for frame in pedestrian_patches:\n",
    "            bbox = np.asarray([frame.bbox.get_bbox_corners()])\n",
    "            bbox = bbox.reshape(4,)\n",
    "            all_bboxes.append(bbox)\n",
    "            confidence = np.asarray([frame.score[0]])\n",
    "            confidences.append(confidence)\n",
    "        confidences = np.array(confidences)\n",
    "        n_confidences = confidences.shape[0]\n",
    "        confidences = confidences.reshape(n_confidences,)\n",
    "        if len(all_bboxes) > 0:\n",
    "            idx = non_max_suppression(np.array(all_bboxes), np.array(confidences), max_output_size=len(all_bboxes), iou_threshold=overlap_thresh)\n",
    "            for i in idx:\n",
    "                nms_patch = pedestrian_patches[i]\n",
    "                nms_patches.append(nms_patch)\n",
    "\n",
    "        # Print the final result\n",
    "        if self.is_debug:\n",
    "            image_test_2 = self.measurements.get_camera_image()\n",
    "            for pedestrian in nms_patches:\n",
    "                bbox = pedestrian.bbox\n",
    "                draw_bbox_to_image(image_test_2, bbox, color=(255,0,0))\n",
    "#             showimage(image_test_2)\n",
    "            self.doa.add_image(\n",
    "                **{\n",
    "                    \"name\": \"Pedestrian patches in the image after NMS\",\n",
    "                    \"description\": \"The patch with highest certainty of containing a pedestrian is shown on the image\\n\",\n",
    "                    \"image\": image_test_2,\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        return [XYZs_filtered, frame_proposals, nms_patches]\n",
    "        \n",
    "    # To get the position of the pedestrian in 3D world from the \"successful\" patch, the initial meshgrid is used.\n",
    "    # The idea is to find the position on the initial meshgrid that generated the pedestrian patch.\n",
    "    def get_T_cam_object(self):\n",
    "\n",
    "        # Get the data from the previous functions\n",
    "        XYZs_filtered, frame_proposals, nms_patches = MyFancyPedestrianDetector.find_pedestrian(self)\n",
    "\n",
    "        # Compare the nms_patches with the initial frame proposals to find the index of the pedestrian patch.\n",
    "        indexes = []\n",
    "        scores = []\n",
    "        for i in range(len(frame_proposals)):\n",
    "            for j in range(len(nms_patches)):\n",
    "                score = nms_patches[j].score\n",
    "                if nms_patches[j].bbox == frame_proposals[i].bbox:\n",
    "                    if i not in indexes:\n",
    "                        indexes.append(i)\n",
    "                    if score not in scores:\n",
    "                        scores.append(score)\n",
    "\n",
    "        # Use the found indexes on the initial grid to find the position of the pedestrian.\n",
    "        pedestrian_pos = []\n",
    "        for i in indexes:\n",
    "            pedestrian_pos.append(XYZs_filtered[i])\n",
    "        pedestrian_pos = np.array(pedestrian_pos)\n",
    "        \n",
    "        # Add a k3d.plot to show pedestrian position in 3D space\n",
    "        if self.is_debug:\n",
    "            plot_1 = k3d.plot()\n",
    "            plot_1 += plot_axes(np.eye(4, dtype=np.float32))\n",
    "            plot_1 += k3d.points(positions=XYZs_filtered.astype(np.float32), point_size=0.2, color=0xff0000)\n",
    "            for pos in pedestrian_pos:\n",
    "                plot_1 += k3d.points(positions=pos.astype(np.float32), point_size=0.5, color=0x0000ff)\n",
    "#             plot_1.display()\n",
    "            self.doa.add_k3d_plot(\n",
    "                **{\n",
    "                    \"name\": \"Pedestrian position in initial meshgrid\",\n",
    "                    \"description\": \"The blue dot shows the pedestrian patch position within all the initial proposals in 3D space.\",\n",
    "                    \"plot\": plot_1,\n",
    "                }\n",
    "            )\n",
    "            \n",
    "        # Create a transformation matrix with a predefined standard rotation and the bbox position just found.\n",
    "        T_cam_object = []\n",
    "        for i in range(len(pedestrian_pos)):\n",
    "            T_cam_object.append(np.array([[0, -1,  0, pedestrian_pos[i, 0]],\n",
    "                                          [0,  0, -1, pedestrian_pos[i, 1]],\n",
    "                                          [1,  0,  0, pedestrian_pos[i, 2]],\n",
    "                                          [0,  0,  0,                   1]]))\n",
    "        \n",
    "        return [T_cam_object, scores]\n",
    "    \n",
    "    # Makes use of all the other function to produce the final dicts\n",
    "    def get_pedestrian_dicts(self):\n",
    "        \n",
    "        # Get the T_cam_object and the score for every detection\n",
    "        T_cam_object, scores = MyFancyPedestrianDetector.get_T_cam_object(self)\n",
    "        \n",
    "        # Creates the dictionary with predifined values for 'label_class' and 'extent_object'\n",
    "        pedestrian_dicts = []\n",
    "        for i in range(len(T_cam_object)):\n",
    "            score = scores[i]\n",
    "            p_dict = {'label_class': 'Pedestrian',\n",
    "                      # For the extent_object, no way was found to precisely detect the size of the pedestrian based on the\n",
    "                      # bounding box dimension since it does not reliably fit on the image, therefore some standard dimensions\n",
    "                      # are used.\n",
    "                      'extent_object': np.array([0.8, 0.8, 1.70]),\n",
    "                      'T_cam_object': T_cam_object[i],\n",
    "                      'score': score[0]}\n",
    "            pedestrian_dicts.append(p_dict)\n",
    "            \n",
    "        return pedestrian_dicts\n",
    "    \n",
    "        \n",
    "\n",
    "pedestrian_detector = MyFancyPedestrianDetector(is_debug=is_debug)\n",
    "\n",
    "# raise NotImplementedError()\n",
    "\n",
    "dataset = Dataset()\n",
    "sequence = dataset.get_custom_sequence(start_index, end_index)\n",
    "\n",
    "# get first measurements object of the sequence\n",
    "measurements = next(iter(sequence))\n",
    "# measurements = sequence[start_index + 55]\n",
    "\n",
    "# feed measurements\n",
    "pedestrian_detector.set_measurements(measurements)\n",
    "\n",
    "pedestrian_dicts = pedestrian_detector.get_pedestrian_dicts()\n",
    "pedestrian_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a72ab9e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01a04ec2d4ed0896d049f67624b17550",
     "grade": true,
     "grade_id": "check-keys",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# make sure each pedestrian_dict has all required keys present\n",
    "required_keys = {\"label_class\", \"extent_object\", \"T_cam_object\", \"score\"}\n",
    "for pedestrian_dict in pedestrian_dicts:\n",
    "    assert required_keys.issubset(set(pedestrian_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b930cc30",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bd305d21bee65d7847083bd52caf9f1",
     "grade": true,
     "grade_id": "check-subclass",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# make sure the pedestrian_detector object is a (duck-typed) PedestrianDetector subclass\n",
    "assert isinstance(pedestrian_dicts, list)\n",
    "assert {\"doa\", \"get_pedestrian_dicts\"}.issubset(set(dir(pedestrian_detector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163e2110",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "745807cdeb5d5529b52880f56fa40bee",
     "grade": true,
     "grade_id": "show-doa-output",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# let's have a look at your debug outputs\n",
    "# show debug outputs\n",
    "[None for i in iter(pedestrian_detector.doa)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d755ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06387ef6c7b9ec3a11d0ec97db1c8d46",
     "grade": true,
     "grade_id": "sample-solution",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a064c3b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "395e7c31dcad92c4b44fb070bfb9a2dc",
     "grade": true,
     "grade_id": "show-doa-sample-solution",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33793717",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e4038c4eaf1e153dc83394e0f93c11b",
     "grade": false,
     "grade_id": "localize-pedestrians-on-whole-sequence",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Localize pedestrians on the whole sequence\n",
    "\n",
    "Please assemble the target structure `frame_pedestrian_dicts` below by iterating over the sequence and obtaining all pedestrian dicts.\n",
    "\n",
    "Along the line we will collect timing information to get a feeling on the efficiency of the approach.\n",
    "We are aware that approaches are hard to compare on different hardware, so *real-time* is not the goal we set here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44505cbb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e70c7d94eab4b5bf9f60eb25847884f1",
     "grade": false,
     "grade_id": "impl-whole-sequence",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from assignment.solution_helpers import DurationAggregator\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sequence = dataset.get_custom_sequence(start_index, end_index)\n",
    "frame_pedestrian_dicts = {\n",
    "    1430: [\n",
    "        {\n",
    "            # ...\n",
    "        },\n",
    "    ]  # frame_index as key. Fill me with pedestrian_dicts using your subclass of PedestrianDetector\n",
    "}\n",
    "\n",
    "is_debug = False\n",
    "pedestrian_detector = None  # overwrite me with your instantiated pedestrian detector class\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Instantiate pedestrian detector\n",
    "pedestrian_detector = MyFancyPedestrianDetector(is_debug=is_debug)\n",
    "\n",
    "# raise NotImplementedError()\n",
    "\n",
    "# log time for running detector on each measurements instance\n",
    "duration_aggregator = DurationAggregator(is_print_durations=True)\n",
    "for measurements in tqdm(duration_aggregator.aggregate_durations(sequence), total=len(sequence)):\n",
    "\n",
    "    pedestrian_detector.set_measurements(measurements)\n",
    "    refined_proposal_dicts_nms = pedestrian_detector.get_pedestrian_dicts()\n",
    "    frame_pedestrian_dicts[measurements.get_index()] = refined_proposal_dicts_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69c49e3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4322370f05210b2f7c311208097eb741",
     "grade": false,
     "grade_id": "mean-duration",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# show mean duration for processing a single frame to answer the question 02a.3 below.\n",
    "assert len(duration_aggregator) == len(sequence)\n",
    "mean_duration_s = duration_aggregator.get_mean_duration_s()\n",
    "print(f\"mean duration: {mean_duration_s:.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c644f982",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8835c32b1629a651d5ecdc013ebbf906",
     "grade": false,
     "grade_id": "efficiency-concepts-timing-task",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "scrolled": false
   },
   "source": [
    "### Q 02a.3 Runtime\n",
    "Please reflect on the mean duration of your algorithm.\n",
    "1. What is the mean duration per timestep of your pedestrian detector on your machine? (see output of cell above)\n",
    "2. How much speed-up would be needed in order to run it 'real-time' within a car given a sensor measurement update rate of 10 Hz?\n",
    "\n",
    "Don't overoptimize: your approach should run at most 30 s per timestep (to keep our inference time during grading manageable), though something around 1-3 s per timestep seems a realistic goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbbe6c5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e441bf2cee69e4063b0ec1e6c93734d1",
     "grade": true,
     "grade_id": "efficiency-concepts-timing-answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": false
   },
   "source": [
    "### A 02a.3\n",
    "**Your answer:** (maximum 150 words)\n",
    "1. The mean duration per timestep is 7.65s with a step in the meshgrid of 1m. With 0.5m the duration per timestep is 15.89 (without cache the first steps may take longer and therefore increase the global average).\n",
    "2. Right now the algorithm computes 0.12 frames per second, while in real time it should compute 10 freames per second. Therefore, the needed speed-up would be x83.3.\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54299e4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ae1fde1ea39cad79520f594761ae250",
     "grade": true,
     "grade_id": "impl-whole-sequence-test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# print(frame_pedestrian_dicts[1430])\n",
    "# check for proper format\n",
    "from assignment.solution_helpers import save_frame_pedestrian_dicts\n",
    "\n",
    "# make sure all frames within the sequence are filled with frame pedestrian dicts\n",
    "assert set(frame_pedestrian_dicts.keys()) == set(sequence.get_indices())\n",
    "\n",
    "# check for type of output\n",
    "for fpds in frame_pedestrian_dicts.values():\n",
    "    for fpd in fpds:\n",
    "        assert {\"label_class\", \"extent_object\", \"T_cam_object\"}.issubset(set(fpd.keys()))\n",
    "        assert fpd[\"T_cam_object\"].shape == (4, 4)\n",
    "        assert fpd[\"label_class\"] == \"Pedestrian\"\n",
    "\n",
    "# use save_frame_pedestrian_dicts with is_dry_run=True to check for serializability\n",
    "is_serializable = True\n",
    "try:\n",
    "    save_frame_pedestrian_dicts(frame_pedestrian_dicts, is_dry_run=True)\n",
    "except TypeError as e:\n",
    "    print(\"Error, frame_pedestrian_dicts is not json serializable: %s\" % str(e))\n",
    "    is_serializable = False\n",
    "if not is_serializable:\n",
    "    assert False, \"See error above\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79b2cde",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1aacbc4362f660a4b374cc92111c9c05",
     "grade": false,
     "grade_id": "quantitative-image",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Quantitative Evaluation (Image Projections)\n",
    "Let's evaluate your detector via comparing the projected 2D bounding boxes of the `frame_pedestrian_dicts` you obtained via your approach against ground truth pedestrian bounding boxes (cf. [Practicum 1](../practicum1/practicum1.ipynb)).\n",
    "Evaluation metrics will be ROC curves, average precision (IoU=0.2) and mean average precision (mAP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e53cec1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb91fec75c05e23bdd7795d3a1aaf987",
     "grade": false,
     "grade_id": "ground-truth-image",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "source": [
    "### Ground Truth bounding boxes (image projections)\n",
    "We fill `gt_bboxes` with a list of pedestrian bounding box coordinates for each image frame (cf. Practicum 1 evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e0a0a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08e3804692e49ea875844c837fdce92b",
     "grade": false,
     "grade_id": "impl-ground-truth-bboxes",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from assignment.evaluation_helpers import get_gt_bboxes\n",
    "gt_bboxes = get_gt_bboxes(sequence)\n",
    "gt_bboxes[0]  # frame 1430"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d0bfb3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54538479a03f1c1f21606e67101cb350",
     "grade": true,
     "grade_id": "test-gt-image",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(gt_bboxes) == len(sequence)\n",
    "assert all(len(bbox) == 4 for bboxes in gt_bboxes for bbox in bboxes)\n",
    "assert gt_bboxes[0][0] == (762, 1746, 958, 1852)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271f0be9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b847d6b99bd22af71936225b3abaf91e",
     "grade": false,
     "grade_id": "predictions-image",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Prediction bounding boxes (image projections)\n",
    "Now, we assemble `sequence_proposals` out of your `frame_pedestrian_dicts` similar to Practicum 1.\n",
    "`sequence_proposals` contain a list of `ImagePatch`es for each frame.\n",
    "Each `ImagePatch` contains the projected 2D bounding box and the detector score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a526b9b1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c353a2f916da7d23cf696dc55c4478d",
     "grade": false,
     "grade_id": "code-predictions-image",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from assignment.evaluation_helpers import get_sequence_proposals\n",
    "sequence_proposals = get_sequence_proposals(sequence, frame_pedestrian_dicts)\n",
    "sequence_proposals[0]  # frame 1430"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24279a8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b52cdc3c01e1f757d08b4ea872f5fbea",
     "grade": true,
     "grade_id": "test-predictions-image",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from practicum1.ImagePatch import ImagePatch\n",
    "\n",
    "assert len(sequence_proposals) == len(gt_bboxes)\n",
    "assert len(sequence_proposals) == len(sequence)\n",
    "# sequence_proposals should be of type ImagePatch and have score of proper shape and range\n",
    "assert all(isinstance(sp, ImagePatch) for sps in sequence_proposals for sp in sps)\n",
    "assert all(\n",
    "    len(sp.score) == 1 for sps in sequence_proposals for sp in sps\n",
    "), \"score as in practicum1 needs to be a one-element list\"\n",
    "assert all(sp.score[0] >= 0.0 for sps in sequence_proposals for sp in sps)\n",
    "assert all(sp.score[0] <= 1.0 for sps in sequence_proposals for sp in sps)\n",
    "for frame_index, pedestrian_dicts in enumerate(frame_pedestrian_dicts.values()):\n",
    "    assert len(sequence_proposals[frame_index]) == len(pedestrian_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550df934",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "364adcf044b0a6fe4ee29f8499041504",
     "grade": false,
     "grade_id": "metrics-dict-image",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Metrics Dict (image projections)\n",
    "We use `generate_metrics_dict` as in Practicum 1 to evaluate `sequence_proposals` against `gt_bboxes` for the given `discrimination_thresholds` and `iou_thresholds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9497db01",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "603d01c34b4f8f4cd10f5b44ff9ab1bf",
     "grade": false,
     "grade_id": "code-metrics-dict-image",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from practicum1.evaluation import generate_metrics_dict\n",
    "\n",
    "discrimination_thresholds = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "iou_thresholds = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "metrics_dict = generate_metrics_dict(sequence_proposals, gt_bboxes, discrimination_thresholds, iou_thresholds)\n",
    "\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b83287",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "470de9d1e9e5be1748df37f5c2cebd50",
     "grade": true,
     "grade_id": "impl-generate-metrics-dict-test",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert set(metrics_dict.keys()) == set(iou_thresholds)\n",
    "assert all(v.shape == (len(discrimination_thresholds), 2) for v in metrics_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c38b5a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "099cb538194a14579fbdf05c6ebb31ff",
     "grade": false,
     "grade_id": "pr-image",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Precision-Recall Curve (image projections)\n",
    "Let's plot the Precision-Recall curve for the IoU threshold of 0.2 (and interactively).\n",
    "See Practicum 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd474638",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99f42f170de8eb2074299cd3d5f67f0a",
     "grade": false,
     "grade_id": "perf-pr-plot-answer",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.practicum1 import plot_pr_curve\n",
    "from ipywidgets import fixed, interact, FloatSlider\n",
    "\n",
    "interact(plot_pr_curve, metrics_dict=fixed(metrics_dict), iou_thresh=FloatSlider(min=0.0, max=1.0, step=0.1, value=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1312fe7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "80865911221ec5bf0d50a72b8d53cb42",
     "grade": false,
     "grade_id": "perf-ap-task",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Average Precision (image projections)\n",
    "What is the `average_precision` for `iou_threshold = 0.2`?\n",
    "Let's reuse code from Practicum 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ae5865",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0fbb5a0549d537ad29789fb3f7849edd",
     "grade": false,
     "grade_id": "impl-ap-answer",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "iou_threshold = 0.2\n",
    "precisions, recalls = metrics_dict[iou_threshold].T\n",
    "average_precision = auc(recalls, precisions)\n",
    "\n",
    "print(f\"Average Precision @ IoU thresh. of {iou_threshold:.01f} = {average_precision * 100:.01f} (image projections)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024104e4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84bb77a46571e17c11da3afeacf76ed7",
     "grade": true,
     "grade_id": "cell-f824fd7bad1fcf70",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert average_precision >= 0.0\n",
    "assert average_precision <= 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7301f1d9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e7d3f2dfc6701b9067fa53033770a110",
     "grade": false,
     "grade_id": "perf-map-task",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Mean Average Precision (image projections)\n",
    "What is the `mean_average_precision` (mAP) of your approach?\n",
    "Let's reuse code from Practicum 1.\n",
    "\n",
    "A basic implementation should achieve an mAP value of at least 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb04a84",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7d4f08efd5520334bec7e4de2074648",
     "grade": false,
     "grade_id": "impl-map-answer",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from practicum1.metrics import mAP\n",
    "\n",
    "mean_average_precision = mAP(metrics_dict)\n",
    "print(f\"Mean Average Precision: {mean_average_precision * 100:.01f} (image projections)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a19615",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b7b3cd094890c5221453c06454146af",
     "grade": true,
     "grade_id": "impl-map-test",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bcc2b3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a0cba13966cf4e829e5981b70f2e97a",
     "grade": false,
     "grade_id": "perf-video-task",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Video (qualitative evaluation)\n",
    "Let's create a video over the whole sequence drawing the projected bounding boxes of all detected 3D pedestrians in ` frame_pedestrian_dicts`.\n",
    "We reuse the function `get_bounding_box_from_object` you implemented in `fa_01a_data_visualization`.\n",
    "The output is a list of images (BGR `np.array`s) called `images_draw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5162ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da482f51d248cb22ae4cb296ded3e8f8",
     "grade": false,
     "grade_id": "impl-video-answer",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from common.visualization import draw_bbox_to_image\n",
    "\n",
    "def draw_pedestrian_bounding_boxes(frame_pedestrian_dicts, sequence):\n",
    "    images_draw = []  # fill me with images of the sequence with pedestrian bounding boxes drawn onto\n",
    "    assert len(frame_pedestrian_dicts) == len(sequence)\n",
    "\n",
    "    for measurements in tqdm(sequence, total=len(sequence)):\n",
    "        pedestrian_dicts = frame_pedestrian_dicts[measurements.get_index()]\n",
    "\n",
    "        image_draw = measurements.get_camera_image()\n",
    "        P2 = measurements.get_camera_projection_matrix()\n",
    "        bboxes = [get_bounding_box_from_object(pd, P2) for pd in pedestrian_dicts]\n",
    "        for bbox in bboxes:\n",
    "            draw_bbox_to_image(image_draw, bbox, color=(0, 255, 0), thickness=3)\n",
    "        images_draw.append(image_draw)\n",
    "    return images_draw\n",
    "\n",
    "\n",
    "images_draw = draw_pedestrian_bounding_boxes(frame_pedestrian_dicts, sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36d63be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32f164acfa62fecd6cef7b0f65c3a6cf",
     "grade": true,
     "grade_id": "impl-video-test",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# make sure we have a video along the whole sequence\n",
    "assert len(images_draw) == len(sequence)\n",
    "# make sure we have images of full resolution and color\n",
    "assert images_draw[0].shape == (1216, 1936, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a475ee1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a2c591e277d340c74faa46b1d4fdf96",
     "grade": false,
     "grade_id": "create-animation",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's visualize the video inline via `create_animation`. This might take a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84c19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.visualization import create_animation\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "anim = create_animation(images_draw)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a6e0cc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "424a40b3101bcddf665f89dad4bbb688",
     "grade": false,
     "grade_id": "perf-bev-visualization",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "scrolled": false
   },
   "source": [
    "# Birds-eye view visualization\n",
    "The above video is handy to analyze the projected bounding boxes.\n",
    "However, it is hard to judge the depth perception of the detected pedestrians.\n",
    "\n",
    "Let's create a birds-eye view plot to judge the distance of the objects to the camera frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db511d7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbb66a00d9f2349ba95613e79a0cec6b",
     "grade": false,
     "grade_id": "bev-predictions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# extract pedestrian positions in birds-eye view for every frame_index from frame_pedestrian_dicts\n",
    "from collections import defaultdict\n",
    "\n",
    "frame_ped_positions = dict()\n",
    "frame_ped_scores = dict()\n",
    "for frame_index, pedestrian_dicts in frame_pedestrian_dicts.items():\n",
    "    frame_ped_positions[frame_index] = []\n",
    "    frame_ped_scores[frame_index] = []\n",
    "    for pedestrian_dict in pedestrian_dicts:\n",
    "        ped_position = pedestrian_dict[\"T_cam_object\"][[0, 2], 3]  # take only xz positions (in camera frame)\n",
    "        frame_ped_positions[frame_index].append(ped_position)\n",
    "        frame_ped_scores[frame_index].append(pedestrian_dict[\"score\"])\n",
    "for frame_index, ped_positions in frame_ped_positions.items():\n",
    "    frame_ped_positions[frame_index] = np.asarray(ped_positions).reshape(-1, 2)\n",
    "for frame_index, ped_scores in frame_ped_scores.items():\n",
    "    frame_ped_scores[frame_index] = np.asarray(ped_scores).reshape(-1, 1)\n",
    "frame_ped_positions[1430], frame_ped_scores[1430]  # frame 1430"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2ad3c1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93ef66dcd0cd21dfb93c72ad72676269",
     "grade": false,
     "grade_id": "bev-gts",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do the same for ground truth pedestrian positions\n",
    "frame_ped_gts = dict()\n",
    "for measurements in sequence:\n",
    "    frame_index = measurements.get_index()\n",
    "    frame_ped_gts[frame_index] = []\n",
    "    # subselect pedestrians\n",
    "    labels_camera = [m for m in measurements.get_labels_camera() if m[\"label_class\"] == \"Pedestrian\"]\n",
    "    for label_camera in labels_camera:\n",
    "        ped_gt = label_camera[\"T_cam_object\"][[0, 2], 3]  # take only xz positions (in camera frame)\n",
    "        frame_ped_gts[frame_index].append(ped_gt)\n",
    "for frame_index, ped_gts in frame_ped_gts.items():\n",
    "    frame_ped_gts[frame_index] = np.asarray(ped_gts)\n",
    "frame_ped_gts[1430]  # frame 1430"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5813f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create interactive plot showing detected pedestrian positions and ground truth positions\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# get bounds for plotting\n",
    "all_ped_positions = np.vstack(list(frame_ped_positions.values()))\n",
    "all_ped_gts = np.vstack(list(frame_ped_gts.values()))\n",
    "all_peds = np.vstack([all_ped_positions, all_ped_gts])\n",
    "xmax, zmax = np.max(np.abs(all_peds), axis=0)  # symmetric\n",
    "xmin, zmin = -xmax, -zmax\n",
    "zmin = 0.0  # make plot start at camera position\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 12))\n",
    "\n",
    "def plot_ped_positions(frame_index):\n",
    "    ax.cla()  # remove content from last frame\n",
    "    ax.set_xlim(left=xmin - 2.0, right=xmax + 2.0)\n",
    "    ax.set_ylim(bottom=zmin, top=zmax + 2.0)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_xlabel(\"x (camera frame)\")\n",
    "    ax.set_ylabel(\"z (camera frame)\")\n",
    "    ax.set_title(f\"frame: {frame_index}\")\n",
    "    ax.grid(True, alpha=0.5)\n",
    "    ax.scatter(0.0, 0.0, color=\"r\")  # camera frame\n",
    "\n",
    "    if frame_ped_gts[frame_index].size > 0:\n",
    "        ax.scatter(\n",
    "            frame_ped_gts[frame_index][:, 0], frame_ped_gts[frame_index][:, 1], color=\"y\", s=500, marker=\"*\", alpha=0.6\n",
    "        )\n",
    "\n",
    "    if frame_ped_positions[frame_index].size > 0:\n",
    "        ax.scatter(frame_ped_positions[frame_index][:, 0], frame_ped_positions[frame_index][:, 1])\n",
    "\n",
    "\n",
    "ani = FuncAnimation(fig, func=plot_ped_positions, frames=list(frame_ped_positions.keys()))\n",
    "plt.close()  # avoid drawing additional figure below animation\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e39bffb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd69fce79a258d552f14a067be94c1c0",
     "grade": false,
     "grade_id": "quantitative-bev",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": false
   },
   "source": [
    "## Quantitative Evaluation (birds-eye view)\n",
    "In the quantitative evaluation (image projections) we have evaluated the implementation of your approach based on projections of the 3D bounding boxes onto the camera image.\n",
    "Remembering your task was to **detect pedestrians in *3D*** and looking at the birds-eye view plot above, we might figure that a measure based on image projections might not be appropriate for evaluating 3D localization.\n",
    "\n",
    "To close that gap, let's quantitatively evaluate your `frame_pedestrian_dicts` against the ground truth pedestrian objects in birds-eye view.\n",
    "We represent both your detections and ground truth objects as circles on the (simplified) ground plane, compared to bounding boxes within the image.\n",
    "The simplified ground plane is span by the XZ plane of the camera frame (thus ignoring y values).\n",
    "We set the variable `radius_m` below to 3 m to be tolerant against inaccurate depth estimates.\n",
    "Similar to the image projection based evaluation, we can associate detections to ground truth objects via overlapping the their circles (IoU, intersection over union).\n",
    "Eventually, this also yields a `metrics_dict` which can be interpreted in the same way as we did above for the image-projection based evaluation.\n",
    "\n",
    "Let's see what average precision (AP) and mean average precision (mAP) we get for the birds-eye view based evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc91c3df",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1dbdcc25e2daa847174766ae54fabe6b",
     "grade": false,
     "grade_id": "sequence-groundplane-proposals",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create sequence ground plane proposals\n",
    "from assignment.evaluation_helpers import get_sequence_proposals_circle\n",
    "\n",
    "sequence_groundplane_proposals = get_sequence_proposals_circle(frame_ped_positions, frame_ped_scores)\n",
    "\n",
    "# ground plane x/y axes correspond to camera frame x/z axes, respectively\n",
    "# so don't be confused by the dict keys 'x' and 'y' below\n",
    "print('The sequence_groundplane_proposals has for every frame a list with a dict for every detection!')\n",
    "print('The list for the first frame is:\\n{}'.format(sequence_groundplane_proposals[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688ad032",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b658dbad327cbf52932079a6b0ca49d",
     "grade": false,
     "grade_id": "gt-sequence-groundplane-proposals",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create ground truth sequence ground plane proposals\n",
    "from assignment.evaluation_helpers import get_GT_sequence_groundplane_proposals\n",
    "\n",
    "GT_sequence_groundplane_proposals = get_GT_sequence_groundplane_proposals(frame_ped_gts)\n",
    "\n",
    "# ground plane x/y axes correspond to camera frame x/z axes, respectively\n",
    "# so don't be confused by the dict keys 'x' and 'y' below\n",
    "print('The GTsequence_groundplane_proposals has for every frame a list with a dict for every pedestrian!')\n",
    "print('The list for the first frame is:\\n{}'.format(GT_sequence_groundplane_proposals[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f0d178",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbcb9310ffd3566ec3095708fecaf6f9",
     "grade": false,
     "grade_id": "metrics-dict-bev",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from assignment.evaluation_helpers import generate_metrics_dict_circle\n",
    "\n",
    "discrimination_thresholds = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "iou_thresholds = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "radius_m = 3.0  # radius of representing circles for overlap computation\n",
    "\n",
    "# generate the metrics_dict from birds-eye view based circles\n",
    "metrics_dict = generate_metrics_dict_circle(sequence_groundplane_proposals,\n",
    "                                            GT_sequence_groundplane_proposals,\n",
    "                                            discrimination_thresholds,\n",
    "                                            iou_thresholds,\n",
    "                                            radius=radius_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2656c42d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44e2643f12bfdc21714f3935a6f66a0c",
     "grade": false,
     "grade_id": "pr-bev",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Precision-Recall Curve (birds-eye view)\n",
    "Let's plot the Precision-Recall curve for the IoU threshold of 0.2 (and interactively).\n",
    "See Practicum 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e66d72",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "259ac297a468a3c58ac4b6db18e0e936",
     "grade": false,
     "grade_id": "plot-pr-bev",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# plot the precision-recall curve\n",
    "interact(plot_pr_curve, metrics_dict=fixed(metrics_dict), iou_thresh=FloatSlider(min=0.0, max=1.0, step=0.1, value=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044c96c8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5aed209b071e9f9fff8a64e40a95448d",
     "grade": false,
     "grade_id": "ap-bev",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Average Precision (birds-eye view)\n",
    "What is the `average_precision` for `iou_threshold = 0.2`?\n",
    "Let's reuse code from Practicum 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a56a74",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee39d079dfca318a670323bb68b20c67",
     "grade": false,
     "grade_id": "code-ap-bev",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "iou_threshold = 0.2\n",
    "precisions, recalls = metrics_dict[iou_threshold].T\n",
    "average_precision = auc(recalls, precisions)\n",
    "\n",
    "print(f\"Average Precision @ IoU thresh. of {iou_threshold:.01f} = {average_precision * 100:.01f} (birds-eye view)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e01d4d1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "863a85a2d38e9e36669334f5d4ac884a",
     "grade": false,
     "grade_id": "map-bev",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Mean Average Precision (birds-eye view)\n",
    "What is the `mean_average_precision` (mAP) of your approach?\n",
    "Let's reuse code from Practicum 1.\n",
    "\n",
    "A basic implementation should achieve an mAP value of at least 5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d99059",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "286f498baec78ca65d609b00f03e9b2f",
     "grade": false,
     "grade_id": "code-map-bev",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from practicum1.metrics import mAP\n",
    "\n",
    "mAP_value = mAP(metrics_dict)\n",
    "\n",
    "print(f'Mean Average Precision: {mAP_value*100:.01f} (birds-eye-view)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2714af",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ffddcc291f7c2d74e178aa9a91245fc",
     "grade": false,
     "grade_id": "interpretation-experimental-results",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q 02a.4 Interpretation of experimental results\n",
    "Please interpret your experimental results:\n",
    "1. Qualitative: How does your approach behave in terms of false positives and false negatives? (video / birds-eye view plot)\n",
    "2. Quantitative: Please discuss the Precision-Recall plot, AP and mAP values in comparison to ideally achievable values. Reflect on the differing values between image-projection based evaluation and birds-eye view based evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26316756",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a11ed142efbf8473deb859d528ea96a5",
     "grade": true,
     "grade_id": "concepts-interpretation-experimental-results-answer",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "### A 02a.4\n",
    "**Your answer:** (maximum 350 words)\n",
    "1. From a qualitative point of view, from the video it is clear to see that the detection is not as precise as it could be. First of all, it struggles to detect further away pedestrians and it concentrates mostly on closer ones. Secondly, it often generates some false positives when analyzing non-conventional structures (e.g. motorbikes). Overall, however, it does detect pedestrian close by and, in terms of safety for a self-driving car, that is the most important feature. Notice that also the false positives are much less of issue if compared to false negatives since in the first case the car stops for no reason, in the second case it might run over a pedestrian.\n",
    "2. For both the image-projection evaluation and the birds-view evaluation the results are overall satisfying. The Precision-Recall curve is slightly better in the image-projection evaluation case rather than the birds-view one, but in both cases it follows the expected shape and can be therefore considered correct. To have a more relatable idea in terms of performance, it is better to analyze the Average Precision metric (AP) that is actually strictly related to the Precision-Recall curve since it represents the area of graph under the curve itself. Obviously, depending on the classifier threshold, the number of false positives and false negatives increases or decreases accordingly: the threshold I am using now, 0.6, I think represents a good middle way and generates good AP values. For the the image-projection the the AP at an IoU threshold of 0.2 is 33.2: the value is lower for higher threshold and does not increase with lower threshold. Differently, for the birds-view evaluation, the AP value at an IoU of 0.2 keeps increasing by lowering the threshold: its value at 0.6 threshold is 28.4, which represents a good tradeoff with the qualitative analysis. To have an even more general idea, though, we can consider the mean Average Precision (mAP): for the image-projection the value is 18.2, while for the birds-view it keeps a little lower at 17.2. Overall, it is understandable that this algorithm performs better in terms of image-projection evaluation rather than birds-view evaluation if the classifier's threshold is set high enough, otherwise, the presence of a lot of false positives, makes the bird-view overperform the image-projection evaluation.\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db193d79",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f033179d55fe9a45bb8559403b681bbf",
     "grade": false,
     "grade_id": "saving-to-disk",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Saving to disk for later usage\n",
    "Now, we're saving the `frame_pedestrian_dicts` to have them accessible to subsequent modules for tracking and motion planning.\n",
    "For now, we don't make use of the saved files, though we might do so in assignments of the coming years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98620be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0fb89c0fe65c4ec1c756fb0f5fb824a7",
     "grade": false,
     "grade_id": "code-saving-to-disk",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from assignment.solution_helpers import save_frame_pedestrian_dicts\n",
    "\n",
    "save_frame_pedestrian_dicts(frame_pedestrian_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6963cf18",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3deb1675c30845fd39bb571f3af5ec93",
     "grade": true,
     "grade_id": "completion",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# make sure file exists\n",
    "import os\n",
    "\n",
    "save_frame_pedestrian_dicts(frame_pedestrian_dicts, is_dry_run=True)\n",
    "assert os.path.exists(os.path.join(os.environ[\"SOURCE_DIR\"], \"assignment\", \"frame-pedestrian-dicts.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec51d7f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "405c9b23135280249828db124e97d4c6",
     "grade": false,
     "grade_id": "future-work",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q 02a.5 Future Work\n",
    "1. How can improve your method even more, i.e., if you had more time at your disposal?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f19fea",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de5c6bc72bd72df074ebbcc670c365a6",
     "grade": true,
     "grade_id": "concepts-future-work-answer",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "### A 02a.5\n",
    "**Your answer:** (maximum 150 words)\n",
    "1. The first thing I would do is trying to optimize the whole algorithm in order to avoid useless computation and make it faster. With a faster algorithm I would be able to run a tighter grid without incurring in excessive computation time per frame. With a tighter grid I would have a higher chance to detect further pedestrian and more detection per pedestrian resulting in a better fit after the selection. A better fit would mean a more precise indication regarding the pedestrian position in each frame, thus increasing the performance of the algorithm. Regarding the other big issue concerning my solution, the dimension of the objects, a possibile way to improve the performance would be using different bounding box sizes for every position and, if they fit well enough, that would allow to get a better idea about the dimension of the objects.\n",
    "\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f4fea6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f4f3becf1c08efc19d55095d3d94c5d",
     "grade": false,
     "grade_id": "great-job",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# GREAT JOB!\n",
    "You've come a long way.\n",
    "You detected pedestrians in 3D with a single camera from a moving vehicle.\n",
    "Are you ready for the last challenge?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
