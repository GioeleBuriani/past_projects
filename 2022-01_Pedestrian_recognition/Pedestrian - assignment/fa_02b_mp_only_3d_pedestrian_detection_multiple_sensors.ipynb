{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69118e9f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e7edb82e9d7e6cb1b96cad9030b692c",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Assignment 02b - (*MP only*) 3D Pedestrian Detection based on Multiple Sensors\n",
    "\n",
    "\n",
    "## Goals\n",
    "The goal of this assignment is the same as in 02a, i.e., obtaining **pedestrian locations in the 3D world (camera reference frame)** on the dataset sequence, but this time you should present a solution which aims to take advantage of **multiple sensors**, i.e. any combination of {camera, radar, LiDAR}.\n",
    "\n",
    "\n",
    "Your approach should work in the following conditions:\n",
    "- minimum distance of pedestrian to camera: 5 m\n",
    "- maximum distance of pedestrian to camera: 40 m\n",
    "- minimum pedestrian height: 1.2 m\n",
    "- maximum pedestrian height: 1.9 m\n",
    "\n",
    "## Input\n",
    "You will work with the custom Sequence of `Dataset` with `start_index=1430` and `end_index=1545`.\n",
    "\n",
    "Some functions you implemented in notebook `fa_02a_3d_pedestrian_detection_single_camera` will be reused in this notebook, so please make sure you finished notebook `fa_02a_3d_pedestrian_detection_single_camera` beforehand.\n",
    "\n",
    "## Output\n",
    "- Plots, visualizations and videos within this notebook\n",
    "- Answers to the questions within this notebook\n",
    "- Evaluation metrics and plots representing the performance of your approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c4eab9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba4989beeb8dd1c8d700e26bd7aacdc2",
     "grade": false,
     "grade_id": "specification-of-intended-solution",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Q 02b.1 Specification of intended solution\n",
    "In this notebook, you have all available sensors to your disposal.\n",
    "\n",
    "Please describe what you will implement to achieve the goal of obtaining 3D pedestrian locations using multiple sensors.\n",
    "Here are some questions to stimulate the specification of your solution:\n",
    "1. What processing steps and building blocks are needed?\n",
    "2. Which concepts from the lectures are you planning to incorporate?\n",
    "3. Which building blocks from the practica and the assignment notebooks before will you be re-using?\n",
    "4. Which intermediate results can you represent in plots/visualizations/tables to show the graders that your intended solution does the right thing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df905b70",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf0e7df92107d10c40007b9572c188f3",
     "grade": true,
     "grade_id": "concepts-intended-solution-answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "### A 02b.1\n",
    "**Your answer:** (maximum 400 words)\n",
    "1. Once again, the first step to face is the detection part. This time aid from other sensors is present, especially the lidar. With the lidar detection it is possible to identify clusters of points that resemble human dimension and build a smart bounding box only around those, and not try a great amount bounding boxes hoping to get the pedestrian. Also, from the 3d analysis, the almost exact position and dimension of the clusters can be registered and paired with the possible patches. The classification part is substantially the same as in the previous case. Finally, the creation of the dictionary is much easier than before since this time it is only necessary to add the positive detections paired with their position and dimension previously registered.\n",
    "2. This time, the knowledge about the lidar comes useful as well, together with all the knowledge necessary for the previous case (`fa_02a_3d_pedestrian_detection_single_camera`). Once again the classification knowledge is not strictly necessary due to the use of a pretrained classifier.\n",
    "3. A part from the detection part, the procedure is very similar to that adopted in `fa_02a_3d_pedestrian_detection_single_camera`, and therefore in parts of `practicum1`. In particular the `PedestrianDetector` class can be directly imported from `fa_02a_3d_pedestrian_detection_single_camera` and other functions such as `filter_points`, `get_bbox` and `find_pedestrian` can be copied with some minor modifications to adapt them to the new scenario. Obviously, the two functions `project_points` and `BoundingBox` from `practicum1` will be used once again.\n",
    "4. Once again, the idea will be showing some significant figures such as k3d plots for the 3D environment analysis and some images with bounding boxes drawn on them for the 2D analysis. With respect to the previous case, this time there will be more 3D analysis since there is access to the lidar point cloud as well.\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d32163c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "010fd6dc2bfaaf71b189f3287817805e",
     "grade": false,
     "grade_id": "proactive-reflection",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q 02b.2 Proactive reflection\n",
    "1. Which assumptions does your intended solution make?\n",
    "2. In which situations might your intended solution fail?\n",
    "3. In which situations is the multi-sensor solution better than the camera-only solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12159655",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52732354534d5873a9f828d293d6562d",
     "grade": true,
     "grade_id": "concepts-proactive-reflection-answer",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "### A 02b.2\n",
    "**Your answer:** (maximum 400 words)\n",
    "1. The main assumption is that lidar and the camera can work in perfect synchrony all the time and that all the transformations between the two are known and constant. The reason why this is fundamental is that every frame the 2D bounding box from the camera is strictly related to the 3D detection of the lidar and therefore the two must work perfectly together.\n",
    "2. The solution cannot be used if there is any sort of delay or malfunctioning in the communication between camera data and lidar data, for the reasons explained above. Also, if real time is crucial, this solution might require an excessive amount of processing that might not make it viable for a real time implementation.\n",
    "3. Probably in almost every situation, since it should be faster (there are much less frame proposals to classify, and more precise in the 3D detection (position and dimension). Obviously, once again, if the communication between lidar and camera is not perfect then this solution does not work and the camera only solution has to be implemented instead.\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fc9a12",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ad4714288cca9a21d9300c00b01ffa7f",
     "grade": false,
     "grade_id": "have-fun",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# now: HAVE FUN & HAPPY CODING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad04f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some magic to ease iterative implementation\n",
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "if ipython:\n",
    "    ipython.magic(\"load_ext autoreload\")\n",
    "    ipython.magic(\"autoreload 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd57563",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dfb06cefb990c107569118b232daf63d",
     "grade": false,
     "grade_id": "ipynb-hint",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If you run into any errors, please make sure that you don't have variable names in the to-be-imported notebooks which consist of pure capital letters (such as `T`, or `XYZ`, but also `P2`).\n",
    "See [ipynb docs](https://ipynb.readthedocs.io/en/stable/#import-only-definitions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd73c1f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "133a547c19f99f7968025bb3b5c65caf",
     "grade": false,
     "grade_id": "impl-own-pedestrian-detector-task",
     "locked": true,
     "points": 9,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "scrolled": false
   },
   "source": [
    "## Your own PedestrianDetector\n",
    "Now it's time to create another subclass of `PedestrianDetector` and detect pedestrians in 3D in the camera frame using **multiple sensors**.\n",
    "Please provide visualizations of a few intermediate steps in order to obtain partial credit for concepts/implementation and to show the graders that your approach provides the intended functionality.\n",
    "\n",
    "We recommend to start with a simple approach and iteratively improve it based on the experience you gain along the line.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ee13e1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6be7e94dcda271c1015674a9ec2d5152",
     "grade": true,
     "grade_id": "impl-own-pedestrian-detector-answer",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create your subclass if PedestrianDetector here\n",
    "# Then instantiate it to an object called `pedestrian_detector`\n",
    "# and feed it with a single measurement of the provided sequence\n",
    "from ipynb.fs.defs.fa_02a_3d_pedestrian_detection_single_camera import PedestrianDetector\n",
    "from common.sequence_loader import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import k3d\n",
    "from common.k3d_helpers import plot_axes, plot_box\n",
    "from ipynb.fs.defs.practicum1 import project_points\n",
    "from sklearn.cluster import DBSCAN\n",
    "from common.visualization import colors_qualitative_k3d\n",
    "from ipynb.fs.defs.practicum1 import BoundingBox\n",
    "from common.visualization import draw_bbox_to_image\n",
    "from common.visualization import showimage\n",
    "from ImagePatch import ImagePatch\n",
    "from BoundingBox import clip_bbox_to_image\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from preprocessing_fns import preprocessing_fn_mobilenet\n",
    "\n",
    "from tensorflow.image import non_max_suppression\n",
    "\n",
    "# try to delete the current instance of pedestrian_detector to avoid running into memory issues\n",
    "# while programming iteratively within this notebook\n",
    "# use try-except as pedestrian_detector does not exist during the first call of this cell\n",
    "try:\n",
    "    del pedestrian_detector\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "is_debug = True\n",
    "\n",
    "# class MyFancyPedestrianDetector(PedestrianDetector):\n",
    "#     ...\n",
    "#\n",
    "# pedestrian_detector = MyFancyPedestrianDetector(is_debug=is_debug)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "class MyFancyPedestrianDetector(PedestrianDetector):\n",
    "    \n",
    "    # Copied the init function from the example above\n",
    "    def __init__(self, is_debug):\n",
    "        super().__init__(is_debug)\n",
    "    \n",
    "        \n",
    "    # Define a function to remove ground from pc_lidar (similar to segment_ground_points from practicum3 but does not work with\n",
    "    # PointCloud)\n",
    "    def remove_ground(pc_lidar, ground_plane):\n",
    "        epsilon = 0.3\n",
    "        points = []\n",
    "        for point in pc_lidar:\n",
    "            if ground_plane[0] * point[0] + ground_plane[1] * point[1] + ground_plane[2] * point[2] + ground_plane[3] > epsilon or ground_plane[0] * point[0] + ground_plane[1] * point[1] + ground_plane[2] * point[2] + ground_plane[3] < -epsilon:\n",
    "                points.append(point)\n",
    "        return np.array(points)\n",
    "    \n",
    "    \n",
    "    # Redefine the filter_points function as in fa_02a_3d_pedestrian_detection_single_camera\n",
    "    def filter_points(image, points, projection_matrix):\n",
    "        # Augment points dimension and project them in 2D\n",
    "        points_aug = np.hstack((points, np.ones((points.shape[0], 1))))\n",
    "        uvs = project_points(projection_matrix, points_aug)\n",
    "        # Filter the points whose 2D projection lands outside the camera image\n",
    "        points_reduced = points[uvs[:, 0] >= 0]\n",
    "        uvs_reduced = uvs[uvs[:, 0] >= 0]\n",
    "        points_reduced = points_reduced[uvs_reduced[:, 0] < image.shape[1]]\n",
    "        uvs_reduced = uvs_reduced[uvs_reduced[:, 0] < image.shape[1]]\n",
    "        points_reduced = points_reduced[uvs_reduced[:, 1] >= 0]\n",
    "        uvs_reduced = uvs_reduced[uvs_reduced[:, 1] >= 0]\n",
    "        points_reduced = points_reduced[uvs_reduced[:, 1] < image.shape[0]]\n",
    "        uvs_reduced = uvs_reduced[uvs_reduced[:, 1] < image.shape[0]]\n",
    "        return points_reduced\n",
    "    \n",
    "    \n",
    "    # Calculate object position by projecting the x, z coordinate on the ground plane\n",
    "    def get_position(x, z, ground_plane):\n",
    "        a, b, c, d = ground_plane\n",
    "        y = (a * x + c * z + d) / (- b)\n",
    "        return np.array([x, y, z])\n",
    "    \n",
    "    \n",
    "    # Create a function to generate the clusters from lidar points that will be analyzed\n",
    "    def get_clusters(self):\n",
    "    \n",
    "        # Import data from measurements\n",
    "        pc_lidar = self.measurements.get_lidar_points()\n",
    "        ground_plane_cf = self.measurements.get_ground_plane()\n",
    "        T_cam_lidar = self.measurements.get_T_camera_lidar()\n",
    "        image = self.measurements.get_camera_image()\n",
    "        projection_matrix = self.measurements.get_camera_projection_matrix()\n",
    "\n",
    "        # Get the lidar points and filter them to only those of interest\n",
    "        pc_lidar_cf = T_cam_lidar.dot(pc_lidar.T).T[:, 0:3]\n",
    "        # Remove ground\n",
    "        pc_lidar_cf = MyFancyPedestrianDetector.remove_ground(pc_lidar_cf, ground_plane_cf)\n",
    "        # Consider only points within 5m and 40m\n",
    "        pc_lidar_cf = pc_lidar_cf[pc_lidar_cf[:, 2] > 5]\n",
    "        pc_lidar_cf = pc_lidar_cf[pc_lidar_cf[:, 2] < 40]\n",
    "        # Consider only points within camera frame\n",
    "        pc_lidar_cf_filtered = MyFancyPedestrianDetector.filter_points(image, pc_lidar_cf, projection_matrix)\n",
    "        \n",
    "        # Plot the only the filtered lidar points\n",
    "        if self.is_debug:\n",
    "            plot = k3d.plot()\n",
    "            plot += plot_axes(np.eye(4, dtype=np.float32))\n",
    "            plot += k3d.points(positions=pc_lidar_cf_filtered.astype(np.float32), point_size=0.1, color=0x0000ff)\n",
    "#             plot.display()\n",
    "            self.doa.add_k3d_plot(\n",
    "                **{\n",
    "                    \"name\": \"Filtered lidar points\",\n",
    "                    \"description\": \"A plot showing only the lidar points of interest for the analysis: no ground plane, within 5m and 40m and within camera frame.\",\n",
    "                    \"plot\": plot,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Cluster the obtained lidar points in order to identify single structures.\n",
    "        # DBSCAN algorithm was chosen since it fits this problem much better than KMeans for several reasons.\n",
    "        clustering = DBSCAN(eps=0.3, min_samples=20).fit(pc_lidar_cf_filtered)\n",
    "        labels = clustering.labels_\n",
    "        n_labels = len(np.unique(labels)) - 1\n",
    "        clusters = []\n",
    "        for i in range(n_labels):\n",
    "            cluster = (pc_lidar_cf_filtered[labels == i])\n",
    "            clusters.append(cluster)\n",
    "        \n",
    "        if self.is_debug:\n",
    "            colors = colors_qualitative_k3d * 5\n",
    "            plot_1 = k3d.plot()\n",
    "            plot_1 += plot_axes(np.eye(4, dtype=np.float32))\n",
    "            for i in range(n_labels):\n",
    "                plot_1 += k3d.points(positions=clusters[i].astype(np.float32), point_size=0.1, color=colors[i])\n",
    "#             plot_1.display()\n",
    "            self.doa.add_k3d_plot(\n",
    "                **{\n",
    "                    \"name\": \"Clustered lidar points\",\n",
    "                    \"description\": \"A plot showing the clustered lidar points with different colours to facilitate cluster identification.\",\n",
    "                    \"plot\": plot_1,\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        return clusters\n",
    "    \n",
    "    \n",
    "    # Filter only the significant clusters and calculate their position, dimension and corners\n",
    "    def filter_clusters(self, epsilon):\n",
    "        \n",
    "        # Get all the clusters with the previous function\n",
    "        clusters = MyFancyPedestrianDetector.get_clusters(self)\n",
    "        \n",
    "        # Create lists\n",
    "        indexes = []\n",
    "        positions = []\n",
    "        dimensions = []\n",
    "        corners_3d = []\n",
    "        \n",
    "        # For each cluster find min and max value for all three dimensions\n",
    "        for i in range(len(clusters)):\n",
    "            cluster = clusters[i]\n",
    "            x_min = np.ndarray.min(cluster[:, 0])\n",
    "            x_max = np.ndarray.max(cluster[:, 0])\n",
    "            y_min = np.ndarray.min(cluster[:, 1])\n",
    "            y_max = np.ndarray.max(cluster[:, 1])\n",
    "            z_min = np.ndarray.min(cluster[:, 2])\n",
    "            z_max = np.ndarray.max(cluster[:, 2])\n",
    "            \n",
    "            # Considering find the middle point along x and z axis and project it on the ground plane to find the exact position\n",
    "            ground_plane_cf = measurements.get_ground_plane()\n",
    "            position = MyFancyPedestrianDetector.get_position((x_max + x_min) / 2, (z_max + z_min) / 2, ground_plane_cf)\n",
    "            \n",
    "            # Calculate the objects' dimension (notice the use of epsilon as a margin)\n",
    "            w = x_max - x_min + epsilon\n",
    "            h = y_max - y_min + 2 * epsilon\n",
    "            l = z_max - z_min + epsilon\n",
    "            dimension = np.array([l, w, h]) \n",
    "            \n",
    "            # Calculate the 3D corners for the future bounding box\n",
    "            corners = []\n",
    "            for j in (-0.1, 0, 0.1):\n",
    "                corner_1 = np.array([position[0] - w/2 - epsilon/2 - j, position[1] + epsilon + j, position[2]])\n",
    "                corner_2 = np.array([position[0] + w/2 + epsilon/2 + j, position[1] + epsilon + j, position[2]])\n",
    "                corner_3 = np.array([position[0] - w/2 - epsilon/2 - j, position[1] - h - j, position[2]])\n",
    "                corner_4 = np.array([position[0] + w/2 + epsilon/2 + j, position[1] - h - j, position[2]])\n",
    "                corner_all = np.array([corner_1, corner_2, corner_3, corner_4])\n",
    "                corners.append(corner_all)\n",
    "            \n",
    "            # Filter only objects that stay within given height range and append position, dimension and corners\n",
    "            if h > 1.2 and h < 1.9 and w < 1.5 and l < 1.5:\n",
    "                indexes.append(i)\n",
    "                positions.append(position)\n",
    "                dimensions.append(dimension)\n",
    "                for corner in corners:\n",
    "                    corners_3d.append(corner)\n",
    "        # Get only clusters within range\n",
    "        good_clusters = [clusters[i] for i in indexes]\n",
    "        \n",
    "        # Plot selected cluster with position and bounding box 3D corners\n",
    "        if self.is_debug:\n",
    "            colors = colors_qualitative_k3d * 5\n",
    "            plot_2 = k3d.plot()\n",
    "            plot_2 += plot_axes(np.eye(4, dtype=np.float32))\n",
    "            for i in range(len(corners_3d)):\n",
    "                plot_2 += k3d.points(positions=good_clusters[int(i/3)].astype(np.float32), point_size=0.1, color=colors[int(i/3)])\n",
    "                plot_2 += k3d.points(positions=positions[int(i/3)].astype(np.float32), point_size=0.3, color=0xff0000)\n",
    "                plot_2 += k3d.points(positions=corners_3d[i].astype(np.float32), point_size=0.2, color=0x00ff00)\n",
    "#             plot_2.display()\n",
    "            self.doa.add_k3d_plot(\n",
    "                **{\n",
    "                    \"name\": \"Filtered clusters\",\n",
    "                    \"description\": \"A plot showing filtered clusters with their measured position (red dots) and 3D bounding box corners (green dots).\",\n",
    "                    \"plot\": plot_2,\n",
    "                }\n",
    "            )\n",
    "            \n",
    "        return positions, dimensions, np.array(corners_3d)\n",
    "    \n",
    "    \n",
    "    # Create bounding boxes and frame proposals based on 3d corners (very similar to what was done in\n",
    "    # fa_02a_3d_pedestrian_detection_single_camera)\n",
    "    def get_bbox(self):\n",
    "        \n",
    "        # Get data from previous function\n",
    "        positions, dimensions, corners_3d = MyFancyPedestrianDetector.filter_clusters(self, 0.25)\n",
    "\n",
    "        # Project the points in 2D\n",
    "        corners_3d_aug = np.dstack((corners_3d, np.ones((corners_3d.shape[0], 4, 1)))).reshape(-1, 4)\n",
    "        projection_matrix = self.measurements.get_camera_projection_matrix()\n",
    "        corners_2d = project_points(projection_matrix, corners_3d_aug)\n",
    "        # Reshape the array to get the corners of each bounding box per row\n",
    "        corners_2d = corners_2d.reshape(-1, 4, 2)\n",
    "\n",
    "        # Create a list with a bounding box for each set of corners\n",
    "        bbox_list = []\n",
    "        for i in range(corners_2d.shape[0]):\n",
    "            bbox = BoundingBox(corners_2d[i, 2, 1].astype(np.int32), corners_2d[i, 2, 0].astype(np.int32), corners_2d[i, 1, 1].astype(np.int32), corners_2d[i, 1, 0].astype(np.int32), from_corners=True)\n",
    "            bbox_list.append(bbox)\n",
    "\n",
    "        # Print the image with all the possible bounding boxes\n",
    "        if self.is_debug:\n",
    "            image_test = self.measurements.get_camera_image()\n",
    "            for i in range(len(bbox_list)):\n",
    "                draw_bbox_to_image(image_test, bbox_list[i])\n",
    "#             showimage(image_test)\n",
    "            self.doa.add_image(\n",
    "                **{\n",
    "                    \"name\": \"All possible bboxes\",\n",
    "                    \"description\": \"The image shows in the 2D environment all the bounding boxes that will be considered for the detection.\\n\",\n",
    "                    \"image\": image_test,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Finally, crate the proposals based on the bounding boxes\n",
    "        image = self.measurements.get_camera_image()\n",
    "        frame_proposals = []\n",
    "        for i in range(len(bbox_list)):\n",
    "            clip_bbox_to_image(bbox_list[i], image.shape[:2])\n",
    "            patch_image = image[bbox_list[i].v:bbox_list[i].v+bbox_list[i].h, bbox_list[i].u:bbox_list[i].u+bbox_list[i].w]\n",
    "            patch = ImagePatch(patch_image, bbox_list[i])\n",
    "            frame_proposals.append(patch)\n",
    "        \n",
    "        return positions, dimensions, frame_proposals\n",
    "    \n",
    "    \n",
    "    # Simply apply the pretrained classifier on the proposals and identify the ones with pedestrian.\n",
    "    # Also this part is almost identical to fa_02a_3d_pedestrian_detection_single_camera\n",
    "    def find_pedestrian(self):\n",
    "        \n",
    "        # First, define the classifier. A pretrained classifier willbe used due to lack of training set.\n",
    "        patch_classifier = tf.keras.models.load_model(os.path.join(os.environ[\"SOURCE_DIR\"], \"practicum1\", \"pedestrian_classifier\"))\n",
    "        \n",
    "        # Get the frame_proposals with get_bbox function\n",
    "        positions, dimensions, frame_proposals = MyFancyPedestrianDetector.get_bbox(self)\n",
    "        \n",
    "        # Preprocess the patches and classify them\n",
    "        frame_patches = np.concatenate([preprocessing_fn_mobilenet(proposal_patch.image) for proposal_patch in frame_proposals], 0)\n",
    "        predictions = patch_classifier.predict(frame_patches)\n",
    "        # Add the score feature of each patch to its description\n",
    "        for i, pred in enumerate(predictions):\n",
    "            frame_proposals[i].score = pred\n",
    "        \n",
    "        # Filter the patches to estract only those representing pedestrian with high certainty\n",
    "        threshold = 0.4\n",
    "        pedestrian_patches = []\n",
    "        idx = []\n",
    "        for i in range(len(frame_proposals)):\n",
    "            if frame_proposals[i].score >= threshold:\n",
    "                pedestrian_patches.append(frame_proposals[i])\n",
    "                idx.append(i)\n",
    "        positions = [positions[int(i/3)] for i in idx]\n",
    "        dimensions = [dimensions[int(i/3)] for i in idx]\n",
    "        \n",
    "        # Reduce the overlapping proposals to a single one using NMS algorithm.\n",
    "        # I had to modify the code from practicum 1 since that only generated one patch even for two distinct pedestrians.\n",
    "        # Notice that in this case it is very uncommon to have overlapping proposals\n",
    "        all_bboxes = []\n",
    "        confidences = []\n",
    "        nms_patches = []\n",
    "        overlap_thresh = 0.01\n",
    "        for frame in pedestrian_patches:\n",
    "            bbox = np.asarray([frame.bbox.get_bbox_corners()])\n",
    "            bbox = bbox.reshape(4,)\n",
    "            all_bboxes.append(bbox)\n",
    "            confidence = np.asarray([frame.score[0]])\n",
    "            confidences.append(confidence)\n",
    "        confidences = np.array(confidences)\n",
    "        n_confidences = confidences.shape[0]\n",
    "        confidences = confidences.reshape(n_confidences,)\n",
    "        if len(all_bboxes) > 0:\n",
    "            idx = non_max_suppression(np.array(all_bboxes), np.array(confidences), max_output_size=len(all_bboxes), iou_threshold=overlap_thresh)\n",
    "            for i in idx:\n",
    "                nms_patch = pedestrian_patches[i]\n",
    "                nms_patches.append(nms_patch)\n",
    "            positions = [positions[i] for i in idx]\n",
    "            dimensions = [dimensions[i] for i in idx]\n",
    "\n",
    "        # Print the final result\n",
    "        if self.is_debug:\n",
    "            image_test_2 = self.measurements.get_camera_image()\n",
    "            for pedestrian in nms_patches:\n",
    "                bbox = pedestrian.bbox\n",
    "                draw_bbox_to_image(image_test_2, bbox, color=(255,0,0))\n",
    "#             showimage(image_test_2)\n",
    "            self.doa.add_image(\n",
    "                **{\n",
    "                    \"name\": \"Pedestrian patches in the image after NMS\",\n",
    "                    \"description\": \"The patch with highest certainty of containing a pedestrian is shown on the image\\n\",\n",
    "                    \"image\": image_test_2,\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        return positions, dimensions, nms_patches\n",
    "    \n",
    "    \n",
    "    # Create the dictionary with all the acquired information\n",
    "    def get_pedestrian_dicts(self):\n",
    "        \n",
    "        # Get information with previous function\n",
    "        positions, dimensions, nms_patches = MyFancyPedestrianDetector.find_pedestrian(self)\n",
    "        \n",
    "        # Create the homogeneous transformation from the object position\n",
    "        T_cam_object = []\n",
    "        positions = np.array(positions)\n",
    "        for i in range(len(positions)):\n",
    "            T_cam_object.append(np.array([[0, -1,  0, positions[i, 0]],\n",
    "                                          [0,  0, -1, positions[i, 1]],\n",
    "                                          [1,  0,  0, positions[i, 2]],\n",
    "                                          [0,  0,  0,              1]]))\n",
    "        \n",
    "        # Create the final disctionary\n",
    "        pedestrian_dicts = []\n",
    "        for i in range(len(nms_patches)):\n",
    "            p_dict = {'label_class': 'Pedestrian',\n",
    "                      # For the extent_object, no way was found to precisely detect the size of the pedestrian based on the\n",
    "                      # bounding box dimension since it does not reliably fit on the image, therefore some standard dimensions\n",
    "                      # are used.\n",
    "                      'extent_object': dimensions[i],\n",
    "                      'T_cam_object': T_cam_object[i],\n",
    "                      'score': nms_patches[i].score[0]}\n",
    "            pedestrian_dicts.append(p_dict)\n",
    "            \n",
    "        return pedestrian_dicts\n",
    "\n",
    "\n",
    "pedestrian_detector = MyFancyPedestrianDetector(is_debug=is_debug)\n",
    "    \n",
    "# raise NotImplementedError()\n",
    "\n",
    "dataset = Dataset()\n",
    "start_index = 1430\n",
    "end_index = 1545\n",
    "sequence = dataset.get_custom_sequence(start_index, end_index)\n",
    "\n",
    "# get first measurements object of the sequence\n",
    "measurements = next(iter(sequence))\n",
    "# measurements = sequence[start_index + 77]\n",
    "\n",
    "# feed measurements\n",
    "pedestrian_detector.set_measurements(measurements)\n",
    "\n",
    "pedestrian_dicts = pedestrian_detector.get_pedestrian_dicts()\n",
    "pedestrian_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a72ab9e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d3de5a96e15462cb62bbb8e373a5b6e",
     "grade": true,
     "grade_id": "check-dict-keys",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# make sure each pedestrian_dict has all required keys present\n",
    "required_keys = {\"label_class\", \"extent_object\", \"T_cam_object\", \"score\"}\n",
    "for pedestrian_dict in pedestrian_dicts:\n",
    "    assert required_keys.issubset(set(pedestrian_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b930cc30",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bd305d21bee65d7847083bd52caf9f1",
     "grade": true,
     "grade_id": "check-subclass",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# make sure the pedestrian_detector object is a (duck-typed) PedestrianDetector subclass\n",
    "assert isinstance(pedestrian_dicts, list)\n",
    "assert {\"doa\", \"get_pedestrian_dicts\"}.issubset(set(dir(pedestrian_detector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163e2110",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "371dbdf763b13b47e72d3afae05cc307",
     "grade": true,
     "grade_id": "output-doa",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# let's have a look at your debug outputs\n",
    "# show debug outputs\n",
    "#\n",
    "# it is important to us, that you create sufficient intermediate results\n",
    "# and also use verbose descriptions of the debug outputs\n",
    "# (as you would use for captions of figures in scientific papers)\n",
    "#\n",
    "# you can toggle scrolling of the output by selecting this cell and 'Cell' > 'Current Outputs' > 'Toggle Scrolling'\n",
    "[None for i in iter(pedestrian_detector.doa)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33793717",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4e0b0348c8d165a7f87d3fc3ca91d6c",
     "grade": false,
     "grade_id": "localize-pedestrians-on-whole-scene",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Localize pedestrians on the whole sequence\n",
    "\n",
    "Please assemble the target structure `frame_pedestrian_dicts` below by iterating over the sequence and obtaining all pedestrian dicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44505cbb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddac356574f7e519b04406452545e6db",
     "grade": false,
     "grade_id": "impl-whole-sequence",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from assignment.solution_helpers import DurationAggregator\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sequence = dataset.get_custom_sequence(start_index, end_index)\n",
    "frame_pedestrian_dicts = {\n",
    "    1430: [\n",
    "        {\n",
    "            # ...\n",
    "        },\n",
    "    ]  # frame_index as key. Fill me with pedestrian_dicts using your subclass of PedestrianDetector\n",
    "}\n",
    "\n",
    "is_debug = False\n",
    "pedestrian_detector = None  # overwrite me with your instantiated pedestrian detector class\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Instantiate pedestrian detector\n",
    "pedestrian_detector = MyFancyPedestrianDetector(is_debug=is_debug)\n",
    "\n",
    "# raise NotImplementedError()\n",
    "\n",
    "# log time for running detector on each measurements instance\n",
    "duration_aggregator = DurationAggregator(is_print_durations=True)\n",
    "for measurements in tqdm(duration_aggregator.aggregate_durations(sequence), total=len(sequence)):\n",
    "\n",
    "    pedestrian_detector.set_measurements(measurements)\n",
    "    refined_proposal_dicts_nms = pedestrian_detector.get_pedestrian_dicts()\n",
    "    frame_pedestrian_dicts[measurements.get_index()] = refined_proposal_dicts_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69c49e3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c93d57344ea7984edcc1f578d03899fb",
     "grade": false,
     "grade_id": "efficiency-timing-task",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(duration_aggregator) == len(sequence)\n",
    "mean_duration_s = duration_aggregator.get_mean_duration_s()\n",
    "print(f\"mean duration: {mean_duration_s:.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c644f982",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7080bfbc1494eee1ba97dadb16012af8",
     "grade": false,
     "grade_id": "impl-timing-task",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "scrolled": false
   },
   "source": [
    "### Q 02b.3 Runtime\n",
    "Please reflect on the mean duration of your algorithm.\n",
    "1. What is the mean duration of your duration on your machine?\n",
    "2. How much speed-up would be needed in order to run it 'real-time' within a car given a sensor measurement update rate of 10 Hz?\n",
    "3. How does the runtime compare against your camera-only detector?\n",
    "\n",
    "Don't overoptimize: your approach should run at most 30 s per timestep (to keep our inference time during grading manageable), though somthing around 1-3 s per timestep seems a realistic goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbbe6c5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d43e72a1d820ab1efa2cc23691077038",
     "grade": true,
     "grade_id": "efficiency-concepts-timing-answer",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": false
   },
   "source": [
    "### A 02b.3\n",
    "**Your answer:** (maximum 150 words)\n",
    "1. The mean duration on my machine is 5.54s per frame.\n",
    "2. Once again the application is much slower than it should be for a real-time implementation, in particular in this case a 55.6x speed-up would be necessary to process the images with an update rate of 10Hz.\n",
    "3. The runtime is slightly faster than the camera-only detector. The reason why it is not much faster, despite having much less frame proposals to classify, is due to the 3D analysis: to create the smart bounding boxes, quite some computations are required (such as clustering) and that unfortunately slows down the whole process.\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54299e4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a3e0f51a2576ebf3231c1ac0be7de69",
     "grade": true,
     "grade_id": "impl-whole-sequence-test",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# check for proper format\n",
    "from assignment.solution_helpers import save_frame_pedestrian_dicts\n",
    "\n",
    "# make sure all frames within the sequence are filled with frame pedestrian dicts\n",
    "assert set(frame_pedestrian_dicts.keys()) == set(sequence.get_indices())\n",
    "\n",
    "# check for type of output\n",
    "for fpds in frame_pedestrian_dicts.values():\n",
    "    for fpd in fpds:\n",
    "        assert {\"label_class\", \"extent_object\", \"T_cam_object\"}.issubset(set(fpd.keys()))\n",
    "        assert fpd[\"T_cam_object\"].shape == (4, 4)\n",
    "        assert fpd[\"label_class\"] == \"Pedestrian\"\n",
    "\n",
    "# use save_frame_pedestrian_dicts with is_dry_run=True to check for serializability\n",
    "is_serializable = True\n",
    "try:\n",
    "    save_frame_pedestrian_dicts(frame_pedestrian_dicts, is_dry_run=True)\n",
    "except TypeError as e:\n",
    "    print(\"Error, frame_pedestrian_dicts is not json serializable: %s\" % str(e))\n",
    "    is_serializable = False\n",
    "if not is_serializable:\n",
    "    assert False, \"See error above\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79b2cde",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36179ba23861e7828ec1ecb8b95777a2",
     "grade": false,
     "grade_id": "quantitaive-evluation",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Quantitative Evaluation (Image Projections)\n",
    "Please evaluate your detector via comparing the projected 2D bounding boxes of the `frame_pedestrian_dicts` you obtained via your approach against ground truth pedestrian bounding boxes (cf. [Practicum 1](../practicum1/practicum1.ipynb)).\n",
    "Evaluation metrics will be ROC curves, average precision (IOU=0.2) and mean average precision (mAP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e53cec1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd1ca45590b5abc305236eff39fa4fa1",
     "grade": false,
     "grade_id": "cell-2245832b86919513",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "source": [
    "### Ground Truth bounding boxes (image projections)\n",
    "Let use compute `gt_bboxes` with a list of pedestrian bounding box coordinates for each frame by reusing our implementation from 02a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e0a0a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8076e95c7bbc9cc1bdea45a55b713ddb",
     "grade": false,
     "grade_id": "impl-ground-truth-bboxes",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.fa_02a_3d_pedestrian_detection_single_camera import get_gt_bboxes\n",
    "\n",
    "gt_bboxes = get_gt_bboxes(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d0bfb3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aef3cb8806286d1706d5a00c5c0220a1",
     "grade": true,
     "grade_id": "impl-ground-truth-bboxes-test",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(gt_bboxes) == len(sequence)\n",
    "assert all(len(bbox) == 4 for bboxes in gt_bboxes for bbox in bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271f0be9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95d35a74117d228898141db5008e76c8",
     "grade": false,
     "grade_id": "prediction-bounding-boxes-image",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Prediction bounding boxes (image projections)\n",
    "Let's assemble `sequence_proposals` out your `frame_pedestrian_dicts` as in 02a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a526b9b1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c2a80b1245b2b32b6202c6da8fac1bc",
     "grade": false,
     "grade_id": "impl-prediction-bounding-boxes",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.fa_02a_3d_pedestrian_detection_single_camera import get_sequence_proposals\n",
    "\n",
    "sequence_proposals = get_sequence_proposals(sequence, frame_pedestrian_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24279a8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ff23e0ea4a7565d9368f52aca745a23",
     "grade": true,
     "grade_id": "impl-prediction-bounding-boxes-test",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from practicum1.ImagePatch import ImagePatch\n",
    "\n",
    "assert len(sequence_proposals) == len(gt_bboxes)\n",
    "# sequence_proposals should be of type ImagePatch and have score of proper shape and range\n",
    "assert all(isinstance(sp, ImagePatch) for sps in sequence_proposals for sp in sps)\n",
    "assert all(\n",
    "    len(sp.score) == 1 for sps in sequence_proposals for sp in sps\n",
    "), \"score as in practicum1 needs to be a one-element list\"\n",
    "assert all(sp.score[0] >= 0.0 for sps in sequence_proposals for sp in sps)\n",
    "assert all(sp.score[0] <= 1.0 for sps in sequence_proposals for sp in sps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550df934",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "364adcf044b0a6fe4ee29f8499041504",
     "grade": false,
     "grade_id": "metrics-dict-image",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Metrics Dict (image projections)\n",
    "We use `generate_metrics_dict` as in Practicum 1 to evaluate `sequence_proposals` against `gt_bboxes` for the given `discrimination_thresholds` and `iou_thresholds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9497db01",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dad82e2728d2b1419579b7387b829ac4",
     "grade": false,
     "grade_id": "metrics-dict-image-impl",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from practicum1.evaluation import generate_metrics_dict\n",
    "\n",
    "discrimination_thresholds = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "iou_thresholds = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "metrics_dict = generate_metrics_dict(sequence_proposals, gt_bboxes, discrimination_thresholds, iou_thresholds)\n",
    "\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b83287",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "470de9d1e9e5be1748df37f5c2cebd50",
     "grade": true,
     "grade_id": "impl-generate-metrics-dict-test",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert set(metrics_dict.keys()) == set(iou_thresholds)\n",
    "assert all(v.shape == (len(discrimination_thresholds), 2) for v in metrics_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c38b5a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6309527fdf285b0854a4a5f805d683f",
     "grade": false,
     "grade_id": "pr-image",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Precision-Recall Curve (image projections)\n",
    "Let's plot the Precision-Recall curve for the IOU threshold of 0.2 (and interactively).\n",
    "See Practicum 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd474638",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0580e279fe859c70076c541a44283df",
     "grade": false,
     "grade_id": "perf-pr-plot-answer",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# insert a precision-recall curve plot here\n",
    "from ipynb.fs.defs.practicum1 import plot_pr_curve\n",
    "from ipywidgets import fixed, interact, FloatSlider\n",
    "\n",
    "interact(plot_pr_curve, metrics_dict=fixed(metrics_dict), iou_thresh=FloatSlider(min=0.0, max=1.0, step=0.1, value=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1312fe7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "81be440c1fda9602df29b5cf0ea7aa77",
     "grade": false,
     "grade_id": "perf-ap-task",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Average Precision (image projections)\n",
    "What is the `average_precision` for `iou_threshold = 0.2`?\n",
    "Feel free to copy your code from 02a.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ae5865",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8d26d594538fc005f606adb681de38b",
     "grade": false,
     "grade_id": "impl-ap-answer",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "iou_threshold = 0.2\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "precisions, recalls = metrics_dict[iou_threshold].T\n",
    "average_precision = auc(recalls, precisions)\n",
    "\n",
    "print(f\"Average Precision @ IoU thresh. of {iou_threshold:.01f} = {average_precision * 100:.01f} (image projections)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024104e4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5775e971c17ad1e4a280d2ff7c1385d5",
     "grade": true,
     "grade_id": "check-ap",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert average_precision >= 0.0\n",
    "assert average_precision <= 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7301f1d9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "891d9587a8e1d68c59799192bd9ce273",
     "grade": false,
     "grade_id": "perf-map-task",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Mean Average Precision (image projections)\n",
    "What is the `mean_average_precision` (mAP) of your approach?\n",
    "\n",
    "A basic implementation should achieve an mAP value of at least 5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb04a84",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7d4f08efd5520334bec7e4de2074648",
     "grade": false,
     "grade_id": "impl-map-answer",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from practicum1.metrics import mAP\n",
    "\n",
    "mean_average_precision = mAP(metrics_dict)\n",
    "print(f\"Mean Average Precision: {mean_average_precision * 100:.01f} (image projections)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d732244",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b7b3cd094890c5221453c06454146af",
     "grade": true,
     "grade_id": "impl-map-test",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-impossible",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "83b5197b2f5b91bc58821a8711ceb40d",
     "grade": false,
     "grade_id": "perf-video-task",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Video (qualitative evaluation)\n",
    "Let's create a video over the whole sequence drawing the projected bounding boxes of all detected 3D pedestrians in `frame_pedestrian_dicts`.\n",
    "We reuse the function `draw_pedestrian_bounding_boxes` we implemented in `fa_02b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-texas",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4bfbc8cc5acd064e1d5ec4c582555c1d",
     "grade": false,
     "grade_id": "impl-video-answer",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.fa_02a_3d_pedestrian_detection_single_camera import draw_pedestrian_bounding_boxes\n",
    "\n",
    "images_draw = draw_pedestrian_bounding_boxes(frame_pedestrian_dicts, sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-overhead",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32f164acfa62fecd6cef7b0f65c3a6cf",
     "grade": true,
     "grade_id": "impl-video-test",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# make sure we have a video along the whole sequence\n",
    "assert len(images_draw) == len(sequence)\n",
    "# make sure we have images of full resolution and color\n",
    "assert images_draw[0].shape == (1216, 1936, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-throw",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1805f54223bf219a391c1438024e0715",
     "grade": false,
     "grade_id": "create-animation-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's visualize the video inline via `create_animation`. This might take a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-calibration",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70d9ffce5d737d252f7838088d550329",
     "grade": false,
     "grade_id": "create-animation-video",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from common.visualization import create_animation\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "anim = create_animation(images_draw)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-click",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ce71e387f8154a5a8d54d7b24b814ed",
     "grade": false,
     "grade_id": "perf-bev-visualization",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Birds-eye view visualization\n",
    "\n",
    "Let's create a birds-eye view plot to judge the distance of the objects to the camera frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bea070",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "15c92e09f0b06905c050775bc89c8edb",
     "grade": false,
     "grade_id": "bev-impl",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# extract pedestrian positions in birds-eye view for every frame_index from frame_pedestrian_dicts\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "frame_ped_positions = dict()\n",
    "frame_ped_scores = dict()\n",
    "for frame_index, pedestrian_dicts in frame_pedestrian_dicts.items():\n",
    "    frame_ped_positions[frame_index] = []\n",
    "    frame_ped_scores[frame_index] = []\n",
    "    for pedestrian_dict in pedestrian_dicts:\n",
    "        ped_position = pedestrian_dict[\"T_cam_object\"][[0, 2], 3]  # take only xz positions (in camera frame)\n",
    "        frame_ped_positions[frame_index].append(ped_position)\n",
    "        frame_ped_scores[frame_index].append(pedestrian_dict[\"score\"])\n",
    "for frame_index, ped_positions in frame_ped_positions.items():\n",
    "    frame_ped_positions[frame_index] = np.asarray(ped_positions).reshape(-1, 2)\n",
    "for frame_index, ped_scores in frame_ped_scores.items():\n",
    "    frame_ped_scores[frame_index] = np.asarray(ped_scores).reshape(-1, 1)\n",
    "frame_ped_positions[1430], frame_ped_scores[1430]  # frame 1430"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de08d1aa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e3e6b85d032d3e4a9b7b0802a034d22",
     "grade": false,
     "grade_id": "frame-ped-gts",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do the same for ground truth pedestrian positions\n",
    "frame_ped_gts = dict()\n",
    "for measurements in sequence:\n",
    "    frame_index = measurements.get_index()\n",
    "    frame_ped_gts[frame_index] = []\n",
    "    # subselect pedestrians\n",
    "    labels_camera = [m for m in measurements.get_labels_camera() if m[\"label_class\"] == \"Pedestrian\"]\n",
    "    for label_camera in labels_camera:\n",
    "        ped_gt = label_camera[\"T_cam_object\"][[0, 2], 3]  # take only xz positions (in camera frame)\n",
    "        frame_ped_gts[frame_index].append(ped_gt)\n",
    "for frame_index, ped_gts in frame_ped_gts.items():\n",
    "    frame_ped_gts[frame_index] = np.asarray(ped_gts)\n",
    "frame_ped_gts[1430]  # frame 1430"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438a1a8a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31ba5da64234dd57b8c23494ca0f1c4c",
     "grade": false,
     "grade_id": "plot-bev",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create interactive plot showing detected pedestrian positions and ground truth positions\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# get bounds for plotting\n",
    "all_ped_positions = np.vstack(list(frame_ped_positions.values()))\n",
    "all_ped_gts = np.vstack(list(frame_ped_gts.values()))\n",
    "all_peds = np.vstack([all_ped_positions, all_ped_gts])\n",
    "xmax, zmax = np.max(np.abs(all_peds), axis=0)  # symmetric\n",
    "xmin, zmin = -xmax, -zmax\n",
    "zmin = 0.0  # make plot start at camera position\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 12))\n",
    "\n",
    "def plot_ped_positions(frame_index):\n",
    "    ax.cla()  # remove content from last frame\n",
    "    ax.set_xlim(left=xmin - 2.0, right=xmax + 2.0)\n",
    "    ax.set_ylim(bottom=zmin, top=zmax + 2.0)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_xlabel(\"x (camera frame)\")\n",
    "    ax.set_ylabel(\"z (camera frame)\")\n",
    "    ax.set_title(f\"frame: {frame_index}\")\n",
    "    ax.grid(True, alpha=0.5)\n",
    "    ax.scatter(0.0, 0.0, color=\"r\")  # camera frame\n",
    "\n",
    "    if frame_ped_gts[frame_index].size > 0:\n",
    "        ax.scatter(\n",
    "            frame_ped_gts[frame_index][:, 0], frame_ped_gts[frame_index][:, 1], color=\"y\", s=500, marker=\"*\", alpha=0.6\n",
    "        )\n",
    "\n",
    "    if frame_ped_positions[frame_index].size > 0:\n",
    "        ax.scatter(frame_ped_positions[frame_index][:, 0], frame_ped_positions[frame_index][:, 1])\n",
    "\n",
    "\n",
    "ani = FuncAnimation(fig, func=plot_ped_positions, frames=list(frame_ped_positions.keys()))\n",
    "plt.close()  # avoid drawing additional figure below animation\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-glasgow",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3ed94be5b3253f41194f899abe9af74",
     "grade": false,
     "grade_id": "quantitative-eval-bev",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Quantitative Evaluation (birds-eye view)\n",
    "\n",
    "Let's see what average precision (AP) and mean average precision (mAP) we get for the birds-eye view based evaluation, as was done in the notebook pedestrian detection with a _single camera_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5066fea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0f237e88d3f4d2e42445efd390cd4ed",
     "grade": false,
     "grade_id": "sequence-groundplane-proposals",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create sequence ground plane proposals\n",
    "# ground plane == XZ plane of camera frame\n",
    "from assignment.evaluation_helpers import get_sequence_proposals_circle\n",
    "\n",
    "sequence_groundplane_proposals = get_sequence_proposals_circle(frame_ped_positions, frame_ped_scores)\n",
    "\n",
    "print('The sequence_groundplane_proposals has for every frame a list with a dict for every detection!')\n",
    "print('The list for the first frame is:\\n{}'.format(sequence_groundplane_proposals[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a40ed8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00e299f5bc0952c906de8173eef39a42",
     "grade": false,
     "grade_id": "gt-sequence-groundplane-proposals",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create ground truth sequence ground plane proposals\n",
    "# ground plane == XZ plane of camera frame\n",
    "from assignment.evaluation_helpers import get_GT_sequence_groundplane_proposals\n",
    "\n",
    "GT_sequence_groundplane_proposals = get_GT_sequence_groundplane_proposals(frame_ped_gts)\n",
    "    \n",
    "print('The GTsequence_groundplane_proposals has for every frame a list with a dict for every pedestrian!')\n",
    "print('The list for the first frame is:\\n{}'.format(GT_sequence_groundplane_proposals[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2909df6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbcb9310ffd3566ec3095708fecaf6f9",
     "grade": false,
     "grade_id": "metrics-dict-bev",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from assignment.evaluation_helpers import generate_metrics_dict_circle\n",
    "\n",
    "discrimination_thresholds = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "iou_thresholds = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "radius_m = 3.0  # radius of representing circles for overlap computation\n",
    "\n",
    "# generate the metrics_dict from birds-eye view based circles\n",
    "metrics_dict = generate_metrics_dict_circle(sequence_groundplane_proposals,\n",
    "                                            GT_sequence_groundplane_proposals,\n",
    "                                            discrimination_thresholds,\n",
    "                                            iou_thresholds,\n",
    "                                            radius=radius_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-notification",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "281eed32fef77dc8b58a5ea87d161695",
     "grade": false,
     "grade_id": "pr-bev",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Precision-Recall Curve (birds-eye view)\n",
    "Run the cell below to plot the Precision-Recall curve for the IoU threshold of 0.2 (and interactively).\n",
    "See Practicum 1 and the notebook pedestrian detection with a _single camera_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31cd5ca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3af851f374c708624559b1800db3100d",
     "grade": false,
     "grade_id": "pr-plot-bev",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# plot the precision-recall curve\n",
    "interact(plot_pr_curve, metrics_dict=fixed(metrics_dict), iou_thresh=FloatSlider(min=0.0, max=1.0, step=0.1, value=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-scoop",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f74682e8df216b3e365a356e4522d1a4",
     "grade": false,
     "grade_id": "perf-ap-bev",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Average Precision (birds-eye view)\n",
    "What is the `average_precision` for `iou_threshold = 0.2`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcb01eb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a72b6164674665c37d8d9f4b5f69ba2e",
     "grade": false,
     "grade_id": "ap-bev",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "iou_threshold = 0.2\n",
    "precisions, recalls = metrics_dict[iou_threshold].T\n",
    "average_precision = auc(recalls, precisions)\n",
    "\n",
    "print(f\"Average Precision @ IoU thresh. of {iou_threshold:.01f} = {average_precision * 100:.01f} (birds-eye view)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-oasis",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "867d8f0b52aaf0ae7278cf7c2595fd57",
     "grade": false,
     "grade_id": "perf-map-bev",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Mean Average Precision (birds-eye view)\n",
    "What is the `mean_average_precision` (mAP) of your approach?\n",
    "Let's reuse code from Practicum 1.\n",
    "\n",
    "A basic implementation should achieve an mAP value of at least 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ca5164",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "475fa5f708dcd83ec74f45e550b5796b",
     "grade": false,
     "grade_id": "map-bev",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from practicum1.metrics import mAP\n",
    "\n",
    "mAP_value = mAP(metrics_dict)\n",
    "\n",
    "print(f'Mean Average Precision: {mAP_value*100:.01f} (birds-eye-view)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2714af",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a6009401bccb071c43ec11e516e7107",
     "grade": false,
     "grade_id": "interpretation-experimental-results",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q 02b.4 Interpretation of experimental results\n",
    "Please interpret your experimental results:\n",
    "1. Qualitative: How does your approach behave in terms of false positives and false negatives? (video / birds-eye view plot)\n",
    "2. Quantitative: Please discuss the Precision-Recall plot, AP and mAP values in comparison to ideally achievable values. Compare the obtained 3D detection performance and associated runtime (this notebook), with the numbers obtained in the single-camera case (previous notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26316756",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9152d6a9c6466b517991eef63d25e07d",
     "grade": true,
     "grade_id": "concepts-interpretation-experimental-results-answer",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "### A 02b.4\n",
    "**Your answer:** (maximum 350 words)\n",
    "1. Like in the camera-only case, the qualititive analysis of the performance is satifying. In particular, however, there are some clear improvementa with respect the the previous scenario. First of all, for the video analysis, it is clear that there is a lower number of false negatives even if the classifier threshold is lower than in the previous case (0.4 against 0.6). The reason is that, since there are much less frame proposals, the risk of triggering a false positive is much smaller. Regarding the birds-eye view, instead, the most clear difference is that, if in the previous case the predictions were more or less close to the actual pedestrians, in this case they are absolutely exact. The reason is that, instead of creating the bouning box and then try to use them to \"catch\" possible pedestrians, this time the bounding boxes were built on top of possible pedestrians and are therefore exact in terms of position.\n",
    "2. Once again the Precision-Recall plots are fairly standard, with an average higher precision rather than recall. In order to better analyze them is once again useful to concentrate on AP and mAP. As expected the values are higher than in the camera-only scenario. For a classifier threshold of 0.4 the AP at an IoU threshold of 0.2 are 34.6 for the image-projection evealueation and 35.3 for the birds-view evaluation. The mAP follows a similar logic, with a value of 25.3 for the image-projection and 32.0 for the birds-view. Contrarily to the previous case, this time the birds-view evaluation is more precise than the image projection. this is probably due to the concept expressed in point 1. about the position being much more exact than before. Also in this case, however, the algorithm cannot identify further pedestrians.\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db193d79",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea96294552b2e8938b4e08c1416d8ca6",
     "grade": false,
     "grade_id": "frame-ped-dicts-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The `frame_pedestrian_dicts` will not be used in successive notebooks.\n",
    "If you need pedestrian positions in further notebooks, please use the serialized output of the camera-only solution.\n",
    "Let's still check for data completeness in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6963cf18",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29b4fe56ea672f32035abea65a21e70b",
     "grade": true,
     "grade_id": "completion",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from assignment.solution_helpers import save_frame_pedestrian_dicts\n",
    "\n",
    "# check for serializability (despite not writing out)\n",
    "save_frame_pedestrian_dicts(frame_pedestrian_dicts, is_dry_run=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec51d7f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8668b23a5f26fdf46ad19bdd40e38b3e",
     "grade": false,
     "grade_id": "future-work",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q 02b.5 Future Work\n",
    "1. How can improve your method even more, i.e., if you had more time at your disposal?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f19fea",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32591ffccc1e5bb2fdcef45b452d19c7",
     "grade": true,
     "grade_id": "concepts-retrospective-reflection-answer",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "### A 02b.5\n",
    "**Your answer:** (maximum 150 words)\n",
    "1. One possible field that can be improved is related to the dimension of the final object. For now the dictionary contains an estimate based on the dimension of the lidar points cluster, with an arbitrary epsilon to reach reasonable final values. This approach can be improved by an analysis on previously known dimensions of objects compared to their clusters' dimensions in order to find a more precise epsilon to use for evaluation. Also, when keeping in mind the real-time implementation, it is clear that the code can be greatly optimized and cleaned up in order to reach a faster implementation closer to the real-time requirements.\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f4fea6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc1eff89eb758d612ff1455103416283",
     "grade": false,
     "grade_id": "great-job",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# GREAT JOB!\n",
    "You've come very far. You detected pedestrian locations in 3D around a moving vehicle from noisy sensor data of multiple sensors.\n",
    "That's a great achievement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
